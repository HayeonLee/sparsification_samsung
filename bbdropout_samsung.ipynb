{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bbdropout_samsung",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayeonLee/sparsification_samsung/blob/master/bbdropout_samsung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD6bQj0iVZM9",
        "colab_type": "text"
      },
      "source": [
        "# Network Sparsification Example : Beta-Bernoulli Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSc0p6Y6VvoL",
        "colab_type": "text"
      },
      "source": [
        "[Adaptive Network Sparsification with Dependent Variational Beta-Bernoulli Dropout](https://arxiv.org/abs/1805.10896)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP0DgMlDV64u",
        "colab_type": "text"
      },
      "source": [
        "## Import Tensorflow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNpLz5neUrqd",
        "colab_type": "code",
        "outputId": "b6bcc7c7-f9de-47ad-f32d-afc6fd170332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#기존에 설치된 다른 버전의 tensorflow를 제거합니다.\n",
        "!pip uninstall tensorboard -y\n",
        "!pip uninstall tensorflow-gpu -y\n",
        "!pip uninstall tensorflow -y\n",
        "#tensorflow gpu 버전을 설치합니다\n",
        "!pip install tensorflow-gpu==1.14"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorboard-1.14.0:\n",
            "  Successfully uninstalled tensorboard-1.14.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "Uninstalling tensorflow-1.14.0:\n",
            "  Successfully uninstalled tensorflow-1.14.0\n",
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/6d/2348df00a34baaabdef0fdb4f46f962f7a8a6720362c26c3a44a249767ea/tensorflow_gpu-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.33.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.7.1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow-gpu==1.14)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/37/e6a7af1c92c5b68fb427f853b06164b56ea92126bcfd87784334ec5e4d42/tensorboard-1.14.0-py2-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.1.7)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.0.post1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==1.14) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.14) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.14) (5.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n",
            "Installing collected packages: tensorboard, tensorflow-gpu\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68oyuhXNWEQ6",
        "colab_type": "code",
        "outputId": "393056b7-9193-42c3-9ea8-389f30bd19cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf # tensorflow를 import해줍니다\n",
        "tf.__version__ # 내가 사용할 tensorflow의 버전을 나타냅니다"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkYfIMpyYdPZ",
        "colab_type": "code",
        "outputId": "d7468fd3-b842-4a90-cf2c-baf1c1e925f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "# pretrain된 lenet의 체크포인트 파일을 가져옵니다.\n",
        "!mkdir -p results/\n",
        "!wget -O lenet_dense_pretrained.zip https://github.com/HayeonLee/sparsification_samsung/blob/master/lenet_dense_pretrained.zip?raw=true\n",
        "!unzip lenet_dense_pretrained.zip -d results/\n",
        "!rm lenet_dense_pretrained.zip\n",
        "!ls\n",
        "!ls results/pretrained/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-24 04:26:18--  https://github.com/HayeonLee/sparsification_samsung/blob/master/lenet_dense_pretrained.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/HayeonLee/sparsification_samsung/raw/master/lenet_dense_pretrained.zip [following]\n",
            "--2019-07-24 04:26:18--  https://github.com/HayeonLee/sparsification_samsung/raw/master/lenet_dense_pretrained.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/HayeonLee/sparsification_samsung/master/lenet_dense_pretrained.zip [following]\n",
            "--2019-07-24 04:26:18--  https://raw.githubusercontent.com/HayeonLee/sparsification_samsung/master/lenet_dense_pretrained.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2080208 (2.0M) [application/zip]\n",
            "Saving to: ‘lenet_dense_pretrained.zip’\n",
            "\n",
            "lenet_dense_pretrai 100%[===================>]   1.98M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-07-24 04:26:19 (22.6 MB/s) - ‘lenet_dense_pretrained.zip’ saved [2080208/2080208]\n",
            "\n",
            "Archive:  lenet_dense_pretrained.zip\n",
            "  inflating: results/pretrained/checkpoint  \n",
            "  inflating: results/pretrained/model.data-00000-of-00001  \n",
            "  inflating: results/pretrained/model.index  \n",
            "  inflating: results/pretrained/model.meta  \n",
            "results  sample_data\n",
            "checkpoint  model.data-00000-of-00001  model.index  model.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR4m7OXGAt8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 라이브러리를 임포트합니다.\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import os\n",
        "from pylab import *\n",
        "import numpy as np\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.contrib.distributions import RelaxedBernoulli\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ0R62L8WvSu",
        "colab_type": "text"
      },
      "source": [
        "## Define the functions and utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFCAzQYqfrzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 자주 쓰는 텐서플로우 함수의 약어를 지정합니다.\n",
        "logit = lambda x: tf.log(x + 1e-20) - tf.log(1-x + 1e-20)\n",
        "softplus = tf.nn.softplus\n",
        "relu = tf.nn.relu\n",
        "\n",
        "dense = tf.layers.dense\n",
        "flatten = tf.contrib.layers.flatten\n",
        "\n",
        "def conv(x, filters, kernel_size=3, strides=1, **kwargs):\n",
        "    return tf.layers.conv2d(x, filters, kernel_size, strides,\n",
        "            data_format='channels_first', **kwargs)\n",
        "\n",
        "def pool(x, **kwargs):\n",
        "    return tf.layers.max_pooling2d(x, 2, 2,\n",
        "            data_format='channels_first', **kwargs)\n",
        "\n",
        "def global_avg_pool(x):\n",
        "    return tf.reduce_mean(x, axis=[2, 3])\n",
        "\n",
        "layer_norm = tf.contrib.layers.layer_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJrRN4ujgQh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils/train.py\n",
        "# 필요한 함수를 정의합니다.\n",
        "def cross_entropy(logits, labels):\n",
        "    return tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels)\n",
        "\n",
        "def weight_decay(decay, var_list=None):\n",
        "    var_list = tf.trainable_variables() if var_list is None else var_list\n",
        "    return decay*tf.add_n([tf.nn.l2_loss(var) for var in var_list])\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "  \n",
        "def digamma_approx(x):\n",
        "# @MISC {1446110,\n",
        "# TITLE = {Approximating the Digamma function},\n",
        "# AUTHOR = {njuffa (https://math.stackexchange.com/users/114200/njuffa)},\n",
        "# HOWPUBLISHED = {Mathematics Stack Exchange},\n",
        "# NOTE = {URL:https://math.stackexchange.com/q/1446110 (version: 2015-09-22)},\n",
        "# EPRINT = {https://math.stackexchange.com/q/1446110},\n",
        "# URL = {https://math.stackexchange.com/q/1446110}}\n",
        "    def digamma_over_one(x):\n",
        "        return tf.log(x + 0.4849142940227510) \\\n",
        "                - 1/(1.0271785180163817*x)\n",
        "    return digamma_over_one(x+1) - 1./x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9RShCp_DDOz",
        "colab": {}
      },
      "source": [
        "# log를 출력하기 위한 함수를 선언합니다.\n",
        "class Accumulator():\n",
        "    def __init__(self, *args):\n",
        "        self.args = args\n",
        "        self.argdict = {}\n",
        "        for i, arg in enumerate(args):\n",
        "            self.argdict[arg] = i\n",
        "        self.sums = [0]*len(args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def accum(self, val):\n",
        "        val = [val] if type(val) is not list else val\n",
        "        val = [v for v in val if v is not None]\n",
        "        assert(len(val) == len(self.args))\n",
        "        for i in range(len(val)):\n",
        "            self.sums[i] += val[i]\n",
        "        self.cnt += 1\n",
        "\n",
        "    def clear(self):\n",
        "        self.sums = [0]*len(self.args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def get(self, arg, avg=True):\n",
        "        i = self.argdict.get(arg, -1)\n",
        "        assert(i is not -1)\n",
        "        return (self.sums[i]/self.cnt if avg else self.sums[i])\n",
        "\n",
        "    def print_(self, header=None, epoch=None, it=None, time=None,\n",
        "            logfile=None, do_not_print=[], as_int=[],\n",
        "            avg=True):\n",
        "        line = '' if header is None else header + ': '\n",
        "        if epoch is not None:\n",
        "            line += ('epoch %d, ' % epoch)\n",
        "        if it is not None:\n",
        "            line += ('iter %d, ' % it)\n",
        "        if time is not None:\n",
        "            line += ('(%.3f secs), ' % time)\n",
        "\n",
        "        args = [arg for arg in self.args if arg not in do_not_print]\n",
        "\n",
        "        for arg in args[:-1]:\n",
        "            val = self.sums[self.argdict[arg]]\n",
        "            if avg:\n",
        "                val /= self.cnt\n",
        "            if arg in as_int:\n",
        "                line += ('%s %d, ' % (arg, int(val)))\n",
        "            else:\n",
        "                line += ('%s %f, ' % (arg, val))\n",
        "        val = self.sums[self.argdict[args[-1]]]\n",
        "        if avg:\n",
        "            val /= self.cnt\n",
        "        if arg in as_int:\n",
        "            line += ('%s %d, ' % (arg, int(val)))\n",
        "        else:\n",
        "            line += ('%s %f' % (args[-1], val))\n",
        "        print(line)\n",
        "\n",
        "        if logfile is not None:\n",
        "            logfile.write(line + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF1WUmZRIdss",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the dataset: MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q21ZNy8bhTpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MNIST_PATH = './mnist'\n",
        "\n",
        "def mnist_input(batch_size):\n",
        "    mnist = input_data.read_data_sets(MNIST_PATH, one_hot=True, validation_size=0)\n",
        "    n_train_batches = mnist.train.num_examples/batch_size\n",
        "    n_test_batches = mnist.test.num_examples/batch_size\n",
        "    return mnist, n_train_batches, n_test_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3lKC6JjHRKM",
        "colab_type": "text"
      },
      "source": [
        "##Create models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg7YnEexflxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fully connected layers로 구성된 lenet을 선언합니다. \n",
        "def lenet_dense(x, y, training, name='lenet', reuse=None,\n",
        "        dropout=None, **dropout_kwargs):\n",
        "    dropout_ = lambda x, subname: x if dropout is None else \\\n",
        "            dropout(x, training, name=name+subname, reuse=reuse,\n",
        "                    **dropout_kwargs)\n",
        "    x = dense(dropout_(x, '/dropout1'), 500, activation=relu,\n",
        "            name=name+'/dense1', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout2'), 300, activation=relu,\n",
        "            name=name+'/dense2', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout3'), 10, name=name+'/dense3', reuse=reuse)\n",
        "\n",
        "    net = {}\n",
        "    all_vars = tf.get_collection('variables', scope=name)\n",
        "    net['qpi_vars'] = [v for v in all_vars if 'qpi_vars' in v.name]\n",
        "    net['pzx_vars'] = [v for v in all_vars if 'pzx_vars' in v.name]\n",
        "    net['weights'] = [v for v in all_vars \\\n",
        "            if 'qpi_vars' not in v.name and 'pzx_vars' not in v.name]\n",
        "\n",
        "    net['cent'] = cross_entropy(x, y)\n",
        "    net['wd'] = weight_decay(1e-4, var_list=net['weights'])\n",
        "    net['acc'] = accuracy(x, y)\n",
        "\n",
        "    prefix = 'train_' if training else 'test_'\n",
        "    net['kl'] = tf.get_collection('kl')\n",
        "    net['pi'] = tf.get_collection(prefix+'pi')\n",
        "    net['n_active'] = tf.get_collection(prefix+'n_active')\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA7g-bi7IP6w",
        "colab_type": "text"
      },
      "source": [
        "## Define the Beta-Bernoulli Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJjoi_-lga15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgamma = tf.lgamma\n",
        "Euler = 0.577215664901532\n",
        "\n",
        "def bbdropout(x, training,\n",
        "        alpha=1e-4, thres=1e-2, a_init=-1., tau=1e-1, center_init=1.0,\n",
        "        approx_digamma=True, scale_kl=None, dep=False,\n",
        "        unit_scale=True, collect=True,\n",
        "        name='bbdropout', reuse=None):\n",
        "\n",
        "    N = tf.shape(x)[0]\n",
        "    K = x.shape[1].value\n",
        "    is_conv = len(x.shape)==4\n",
        "    log = lambda x: tf.log(x + 1e-20)\n",
        "\n",
        "    with tf.variable_scope(name+'/qpi_vars', reuse=reuse):\n",
        "        with tf.device('/cpu:0'):\n",
        "            a = softplus(tf.get_variable('a_uc', shape=[K],\n",
        "                initializer=tf.constant_initializer(a_init)))\n",
        "            b = softplus(tf.get_variable('b_uc', shape=[K]))\n",
        "\n",
        "    _digamma = digamma_approx \n",
        "    kl = (a-alpha)/a * (-Euler - _digamma(b) - 1/b) \\\n",
        "            + log(a*b) - log(alpha) - (b-1)/b\n",
        "    pi = (1 - tf.random_uniform([K])**(1/b))**(1/a) if training else \\\n",
        "            b*tf.exp(lgamma(1+1/a) + lgamma(b) - lgamma(1+1/a+b))\n",
        "    \n",
        "    if training:\n",
        "        z = RelaxedBernoulli(tau, logits=logit(pi)).sample(N)\n",
        "    else:\n",
        "        pi_ = tf.where(tf.greater(pi, thres), pi, tf.zeros_like(pi))\n",
        "        z = tf.tile(tf.expand_dims(pi_, 0), [N, 1])\n",
        "    n_active = tf.reduce_sum(tf.cast(tf.greater(pi, thres), tf.int32))\n",
        "\n",
        "    if scale_kl is None:\n",
        "        kl = tf.reduce_sum(kl)\n",
        "    else:\n",
        "        kl = scale_kl * tf.reduce_mean(kl)\n",
        "\n",
        "    if collect:\n",
        "        if reuse is not True:\n",
        "            tf.add_to_collection('kl', kl)\n",
        "        prefix = 'train_' if training else 'test_'\n",
        "        tf.add_to_collection(prefix+'pi', pi)\n",
        "        tf.add_to_collection(prefix+'n_active', n_active)\n",
        "\n",
        "    z = tf.reshape(z, ([-1, K, 1, 1] if is_conv else [-1, K]))\n",
        "    return x*z\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InEG2TA3Iilj",
        "colab_type": "text"
      },
      "source": [
        "## Let's run the code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz6djPqUWps9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "dcb3214b-a5f2-4670-b64c-59773c821aca"
      },
      "source": [
        "tf.reset_default_graph() # 기존의 그려진 텐서플로우 그래프를 제거합니다.\n",
        "\n",
        "pretraindir = './results/pretrained' \n",
        "savedir = './results/bbdropout/sample_run' \n",
        "if not os.path.isdir(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "batch_size = 100\n",
        "n_epochs = 60\n",
        "save_freq = 20\n",
        "mnist, n_train_batches, n_test_batches = mnist_input(batch_size)\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "N = mnist.train.num_examples\n",
        "dropout = bbdropout\n",
        "net = lenet_dense(x, y, True, dropout=dropout)\n",
        "tnet = lenet_dense(x, y, False, reuse=True, dropout=dropout)\n",
        "\n",
        "def train():\n",
        "    loss = net['cent'] + tf.add_n(net['kl'])/float(N) + net['wd'] \n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    bdr = [int(n_train_batches*(n_epochs-1)*r) for r in [0.5, 0.75]]\n",
        "    vals = [1e-2, 1e-3, 1e-4]\n",
        "    lr = tf.train.piecewise_constant(tf.cast(global_step, tf.int32), bdr, vals)\n",
        "    train_op1 = tf.train.AdamOptimizer(lr).minimize(loss,\n",
        "            var_list=net['qpi_vars'], global_step=global_step)\n",
        "    train_op2 = tf.train.AdamOptimizer(0.1*lr).minimize(loss,\n",
        "            var_list=net['weights'])\n",
        "    train_op = tf.group(train_op1, train_op2)\n",
        "\n",
        "    pretrain_saver = tf.train.Saver(net['weights'])\n",
        "    saver = tf.train.Saver(net['weights']+net['qpi_vars'])\n",
        "    logfile = open(os.path.join(savedir, 'train.log'), 'w', 0)\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    pretrain_saver.restore(sess, os.path.join(pretraindir, 'model'))\n",
        "\n",
        "    train_logger = Accumulator('cent', 'acc')\n",
        "    train_to_run = [train_op, net['cent'], net['acc']]\n",
        "    test_logger = Accumulator('cent', 'acc')\n",
        "    test_to_run = [tnet['cent'], tnet['acc']]\n",
        "    for i in range(n_epochs):\n",
        "        line = 'Epoch %d start, learning rate %f' % (i+1, sess.run(lr))\n",
        "        print(line)\n",
        "        logfile.write(line + '\\n')\n",
        "        train_logger.clear()\n",
        "        start = time.time()\n",
        "        for j in range(n_train_batches):\n",
        "            bx, by = mnist.train.next_batch(batch_size)\n",
        "            train_logger.accum(sess.run(train_to_run, {x:bx, y:by}))\n",
        "        train_logger.print_(header='train', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "\n",
        "        test_logger.clear()\n",
        "        for j in range(n_test_batches):\n",
        "            bx, by = mnist.test.next_batch(batch_size)\n",
        "            test_logger.accum(sess.run(test_to_run, {x:bx, y:by}))\n",
        "        test_logger.print_(header='test', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "        #line = 'kl: ' + str(sess.run(tnet['kl'])) + '\\n'\n",
        "        line += '\\nn_active: ' + str(sess.run(tnet['n_active'])) + '\\n'\n",
        "        print(line)\n",
        "        logfile.write(line+'\\n')\n",
        "\n",
        "        if (i+1)% save_freq == 0:\n",
        "            saver.save(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    logfile.close()\n",
        "    saver.save(sess, os.path.join(savedir, 'model'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_0exUrqhel8",
        "colab_type": "code",
        "outputId": "8be4a28c-485b-4124-f3dd-a6066f1eacfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 start, learning rate 0.010000\n",
            "train: epoch 1, (4.767 secs), cent 0.407368, acc 0.879783\n",
            "test: epoch 1, (5.067 secs), cent 0.105316, acc 0.967300\n",
            "Epoch 1 start, learning rate 0.010000\n",
            "n_active: [519, 469, 296]\n",
            "\n",
            "Epoch 2 start, learning rate 0.010000\n",
            "train: epoch 2, (4.068 secs), cent 0.165206, acc 0.948550\n",
            "test: epoch 2, (4.315 secs), cent 0.080664, acc 0.973900\n",
            "Epoch 2 start, learning rate 0.010000\n",
            "n_active: [507, 464, 296]\n",
            "\n",
            "Epoch 3 start, learning rate 0.010000\n",
            "train: epoch 3, (4.089 secs), cent 0.120991, acc 0.962133\n",
            "test: epoch 3, (4.331 secs), cent 0.065733, acc 0.979900\n",
            "Epoch 3 start, learning rate 0.010000\n",
            "n_active: [502, 461, 296]\n",
            "\n",
            "Epoch 4 start, learning rate 0.010000\n",
            "train: epoch 4, (4.059 secs), cent 0.102603, acc 0.968150\n",
            "test: epoch 4, (4.296 secs), cent 0.060035, acc 0.980400\n",
            "Epoch 4 start, learning rate 0.010000\n",
            "n_active: [496, 456, 296]\n",
            "\n",
            "Epoch 5 start, learning rate 0.010000\n",
            "train: epoch 5, (4.146 secs), cent 0.087195, acc 0.972700\n",
            "test: epoch 5, (4.397 secs), cent 0.056786, acc 0.981600\n",
            "Epoch 5 start, learning rate 0.010000\n",
            "n_active: [488, 455, 296]\n",
            "\n",
            "Epoch 6 start, learning rate 0.010000\n",
            "train: epoch 6, (4.216 secs), cent 0.077088, acc 0.976117\n",
            "test: epoch 6, (4.470 secs), cent 0.055389, acc 0.983200\n",
            "Epoch 6 start, learning rate 0.010000\n",
            "n_active: [483, 453, 296]\n",
            "\n",
            "Epoch 7 start, learning rate 0.010000\n",
            "train: epoch 7, (4.174 secs), cent 0.072104, acc 0.977150\n",
            "test: epoch 7, (4.406 secs), cent 0.055888, acc 0.983000\n",
            "Epoch 7 start, learning rate 0.010000\n",
            "n_active: [478, 452, 296]\n",
            "\n",
            "Epoch 8 start, learning rate 0.010000\n",
            "train: epoch 8, (4.179 secs), cent 0.066732, acc 0.978567\n",
            "test: epoch 8, (4.424 secs), cent 0.054670, acc 0.982100\n",
            "Epoch 8 start, learning rate 0.010000\n",
            "n_active: [477, 450, 296]\n",
            "\n",
            "Epoch 9 start, learning rate 0.010000\n",
            "train: epoch 9, (4.204 secs), cent 0.061943, acc 0.980117\n",
            "test: epoch 9, (4.435 secs), cent 0.050333, acc 0.983700\n",
            "Epoch 9 start, learning rate 0.010000\n",
            "n_active: [471, 446, 296]\n",
            "\n",
            "Epoch 10 start, learning rate 0.010000\n",
            "train: epoch 10, (4.157 secs), cent 0.061612, acc 0.980050\n",
            "test: epoch 10, (4.387 secs), cent 0.050370, acc 0.983700\n",
            "Epoch 10 start, learning rate 0.010000\n",
            "n_active: [466, 437, 296]\n",
            "\n",
            "Epoch 11 start, learning rate 0.010000\n",
            "train: epoch 11, (4.163 secs), cent 0.055591, acc 0.982417\n",
            "test: epoch 11, (4.393 secs), cent 0.053148, acc 0.983900\n",
            "Epoch 11 start, learning rate 0.010000\n",
            "n_active: [454, 427, 295]\n",
            "\n",
            "Epoch 12 start, learning rate 0.010000\n",
            "train: epoch 12, (4.188 secs), cent 0.055490, acc 0.981833\n",
            "test: epoch 12, (4.429 secs), cent 0.052601, acc 0.983800\n",
            "Epoch 12 start, learning rate 0.010000\n",
            "n_active: [449, 411, 294]\n",
            "\n",
            "Epoch 13 start, learning rate 0.010000\n",
            "train: epoch 13, (4.173 secs), cent 0.050264, acc 0.983550\n",
            "test: epoch 13, (4.422 secs), cent 0.050996, acc 0.983500\n",
            "Epoch 13 start, learning rate 0.010000\n",
            "n_active: [444, 391, 293]\n",
            "\n",
            "Epoch 14 start, learning rate 0.010000\n",
            "train: epoch 14, (4.229 secs), cent 0.047800, acc 0.984950\n",
            "test: epoch 14, (4.467 secs), cent 0.051714, acc 0.984300\n",
            "Epoch 14 start, learning rate 0.010000\n",
            "n_active: [439, 374, 288]\n",
            "\n",
            "Epoch 15 start, learning rate 0.010000\n",
            "train: epoch 15, (4.195 secs), cent 0.045922, acc 0.985383\n",
            "test: epoch 15, (4.435 secs), cent 0.055378, acc 0.983900\n",
            "Epoch 15 start, learning rate 0.010000\n",
            "n_active: [432, 356, 284]\n",
            "\n",
            "Epoch 16 start, learning rate 0.010000\n",
            "train: epoch 16, (4.171 secs), cent 0.046104, acc 0.985150\n",
            "test: epoch 16, (4.410 secs), cent 0.056227, acc 0.982000\n",
            "Epoch 16 start, learning rate 0.010000\n",
            "n_active: [425, 337, 281]\n",
            "\n",
            "Epoch 17 start, learning rate 0.010000\n",
            "train: epoch 17, (4.203 secs), cent 0.045122, acc 0.985000\n",
            "test: epoch 17, (4.461 secs), cent 0.047194, acc 0.985200\n",
            "Epoch 17 start, learning rate 0.010000\n",
            "n_active: [420, 320, 275]\n",
            "\n",
            "Epoch 18 start, learning rate 0.010000\n",
            "train: epoch 18, (4.202 secs), cent 0.041350, acc 0.986150\n",
            "test: epoch 18, (4.439 secs), cent 0.048037, acc 0.985100\n",
            "Epoch 18 start, learning rate 0.010000\n",
            "n_active: [418, 309, 264]\n",
            "\n",
            "Epoch 19 start, learning rate 0.010000\n",
            "train: epoch 19, (4.204 secs), cent 0.039984, acc 0.986967\n",
            "test: epoch 19, (4.438 secs), cent 0.056546, acc 0.983300\n",
            "Epoch 19 start, learning rate 0.010000\n",
            "n_active: [417, 302, 250]\n",
            "\n",
            "Epoch 20 start, learning rate 0.010000\n",
            "train: epoch 20, (4.217 secs), cent 0.040685, acc 0.986933\n",
            "test: epoch 20, (4.452 secs), cent 0.050858, acc 0.983500\n",
            "Epoch 20 start, learning rate 0.010000\n",
            "n_active: [411, 292, 243]\n",
            "\n",
            "Epoch 21 start, learning rate 0.010000\n",
            "train: epoch 21, (4.105 secs), cent 0.038946, acc 0.987550\n",
            "test: epoch 21, (4.343 secs), cent 0.047813, acc 0.985400\n",
            "Epoch 21 start, learning rate 0.010000\n",
            "n_active: [406, 279, 233]\n",
            "\n",
            "Epoch 22 start, learning rate 0.010000\n",
            "train: epoch 22, (4.216 secs), cent 0.036272, acc 0.988217\n",
            "test: epoch 22, (4.442 secs), cent 0.052161, acc 0.984200\n",
            "Epoch 22 start, learning rate 0.010000\n",
            "n_active: [399, 265, 223]\n",
            "\n",
            "Epoch 23 start, learning rate 0.010000\n",
            "train: epoch 23, (4.158 secs), cent 0.037711, acc 0.987600\n",
            "test: epoch 23, (4.417 secs), cent 0.050445, acc 0.985100\n",
            "Epoch 23 start, learning rate 0.010000\n",
            "n_active: [394, 256, 217]\n",
            "\n",
            "Epoch 24 start, learning rate 0.010000\n",
            "train: epoch 24, (4.362 secs), cent 0.036046, acc 0.988317\n",
            "test: epoch 24, (4.614 secs), cent 0.050200, acc 0.984600\n",
            "Epoch 24 start, learning rate 0.010000\n",
            "n_active: [393, 247, 206]\n",
            "\n",
            "Epoch 25 start, learning rate 0.010000\n",
            "train: epoch 25, (4.356 secs), cent 0.035327, acc 0.988200\n",
            "test: epoch 25, (4.623 secs), cent 0.049025, acc 0.986300\n",
            "Epoch 25 start, learning rate 0.010000\n",
            "n_active: [388, 242, 202]\n",
            "\n",
            "Epoch 26 start, learning rate 0.010000\n",
            "train: epoch 26, (4.159 secs), cent 0.033896, acc 0.989133\n",
            "test: epoch 26, (4.389 secs), cent 0.046686, acc 0.986200\n",
            "Epoch 26 start, learning rate 0.010000\n",
            "n_active: [386, 240, 193]\n",
            "\n",
            "Epoch 27 start, learning rate 0.010000\n",
            "train: epoch 27, (4.203 secs), cent 0.033559, acc 0.988867\n",
            "test: epoch 27, (4.429 secs), cent 0.047949, acc 0.985300\n",
            "Epoch 27 start, learning rate 0.010000\n",
            "n_active: [379, 232, 186]\n",
            "\n",
            "Epoch 28 start, learning rate 0.010000\n",
            "train: epoch 28, (4.032 secs), cent 0.033821, acc 0.989033\n",
            "test: epoch 28, (4.265 secs), cent 0.049790, acc 0.985600\n",
            "Epoch 28 start, learning rate 0.010000\n",
            "n_active: [378, 222, 183]\n",
            "\n",
            "Epoch 29 start, learning rate 0.010000\n",
            "train: epoch 29, (4.136 secs), cent 0.032402, acc 0.989583\n",
            "test: epoch 29, (4.386 secs), cent 0.047458, acc 0.985900\n",
            "Epoch 29 start, learning rate 0.010000\n",
            "n_active: [375, 219, 177]\n",
            "\n",
            "Epoch 30 start, learning rate 0.010000\n",
            "train: epoch 30, (4.127 secs), cent 0.026727, acc 0.991400\n",
            "test: epoch 30, (4.364 secs), cent 0.044139, acc 0.987100\n",
            "Epoch 30 start, learning rate 0.010000\n",
            "n_active: [374, 216, 174]\n",
            "\n",
            "Epoch 31 start, learning rate 0.001000\n",
            "train: epoch 31, (4.119 secs), cent 0.019111, acc 0.994200\n",
            "test: epoch 31, (4.350 secs), cent 0.042588, acc 0.987000\n",
            "Epoch 31 start, learning rate 0.001000\n",
            "n_active: [373, 215, 174]\n",
            "\n",
            "Epoch 32 start, learning rate 0.001000\n",
            "train: epoch 32, (4.145 secs), cent 0.017879, acc 0.994417\n",
            "test: epoch 32, (4.389 secs), cent 0.042553, acc 0.987400\n",
            "Epoch 32 start, learning rate 0.001000\n",
            "n_active: [372, 215, 174]\n",
            "\n",
            "Epoch 33 start, learning rate 0.001000\n",
            "train: epoch 33, (4.093 secs), cent 0.017188, acc 0.994633\n",
            "test: epoch 33, (4.338 secs), cent 0.043098, acc 0.987500\n",
            "Epoch 33 start, learning rate 0.001000\n",
            "n_active: [372, 215, 173]\n",
            "\n",
            "Epoch 34 start, learning rate 0.001000\n",
            "train: epoch 34, (4.220 secs), cent 0.016844, acc 0.995067\n",
            "test: epoch 34, (4.463 secs), cent 0.042912, acc 0.987400\n",
            "Epoch 34 start, learning rate 0.001000\n",
            "n_active: [370, 213, 170]\n",
            "\n",
            "Epoch 35 start, learning rate 0.001000\n",
            "train: epoch 35, (4.177 secs), cent 0.015481, acc 0.995650\n",
            "test: epoch 35, (4.416 secs), cent 0.043134, acc 0.987000\n",
            "Epoch 35 start, learning rate 0.001000\n",
            "n_active: [370, 211, 170]\n",
            "\n",
            "Epoch 36 start, learning rate 0.001000\n",
            "train: epoch 36, (4.200 secs), cent 0.015814, acc 0.995267\n",
            "test: epoch 36, (4.433 secs), cent 0.043086, acc 0.987600\n",
            "Epoch 36 start, learning rate 0.001000\n",
            "n_active: [369, 211, 169]\n",
            "\n",
            "Epoch 37 start, learning rate 0.001000\n",
            "train: epoch 37, (4.134 secs), cent 0.016064, acc 0.995150\n",
            "test: epoch 37, (4.371 secs), cent 0.042456, acc 0.987500\n",
            "Epoch 37 start, learning rate 0.001000\n",
            "n_active: [369, 208, 166]\n",
            "\n",
            "Epoch 38 start, learning rate 0.001000\n",
            "train: epoch 38, (4.106 secs), cent 0.016176, acc 0.995350\n",
            "test: epoch 38, (4.353 secs), cent 0.041884, acc 0.987700\n",
            "Epoch 38 start, learning rate 0.001000\n",
            "n_active: [369, 208, 163]\n",
            "\n",
            "Epoch 39 start, learning rate 0.001000\n",
            "train: epoch 39, (4.110 secs), cent 0.015719, acc 0.995350\n",
            "test: epoch 39, (4.355 secs), cent 0.042292, acc 0.987500\n",
            "Epoch 39 start, learning rate 0.001000\n",
            "n_active: [369, 205, 163]\n",
            "\n",
            "Epoch 40 start, learning rate 0.001000\n",
            "train: epoch 40, (4.130 secs), cent 0.016441, acc 0.994917\n",
            "test: epoch 40, (4.362 secs), cent 0.041793, acc 0.987600\n",
            "Epoch 40 start, learning rate 0.001000\n",
            "n_active: [368, 202, 162]\n",
            "\n",
            "Epoch 41 start, learning rate 0.001000\n",
            "train: epoch 41, (4.309 secs), cent 0.016213, acc 0.995300\n",
            "test: epoch 41, (4.549 secs), cent 0.041666, acc 0.986900\n",
            "Epoch 41 start, learning rate 0.001000\n",
            "n_active: [367, 201, 160]\n",
            "\n",
            "Epoch 42 start, learning rate 0.001000\n",
            "train: epoch 42, (4.254 secs), cent 0.015662, acc 0.995350\n",
            "test: epoch 42, (4.488 secs), cent 0.042538, acc 0.987100\n",
            "Epoch 42 start, learning rate 0.001000\n",
            "n_active: [367, 201, 157]\n",
            "\n",
            "Epoch 43 start, learning rate 0.001000\n",
            "train: epoch 43, (4.124 secs), cent 0.015976, acc 0.994850\n",
            "test: epoch 43, (4.370 secs), cent 0.041736, acc 0.987000\n",
            "Epoch 43 start, learning rate 0.001000\n",
            "n_active: [367, 201, 156]\n",
            "\n",
            "Epoch 44 start, learning rate 0.001000\n",
            "train: epoch 44, (4.197 secs), cent 0.016065, acc 0.995233\n",
            "test: epoch 44, (4.440 secs), cent 0.042033, acc 0.987800\n",
            "Epoch 44 start, learning rate 0.001000\n",
            "n_active: [367, 199, 153]\n",
            "\n",
            "Epoch 45 start, learning rate 0.001000\n",
            "train: epoch 45, (4.118 secs), cent 0.014850, acc 0.995683\n",
            "test: epoch 45, (4.349 secs), cent 0.042475, acc 0.987400\n",
            "Epoch 45 start, learning rate 0.001000\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 46 start, learning rate 0.000100\n",
            "train: epoch 46, (4.101 secs), cent 0.014707, acc 0.995817\n",
            "test: epoch 46, (4.333 secs), cent 0.042258, acc 0.987800\n",
            "Epoch 46 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 47 start, learning rate 0.000100\n",
            "train: epoch 47, (4.105 secs), cent 0.014360, acc 0.995917\n",
            "test: epoch 47, (4.356 secs), cent 0.042133, acc 0.987900\n",
            "Epoch 47 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 48 start, learning rate 0.000100\n",
            "train: epoch 48, (4.169 secs), cent 0.014830, acc 0.995400\n",
            "test: epoch 48, (4.413 secs), cent 0.042028, acc 0.987700\n",
            "Epoch 48 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 49 start, learning rate 0.000100\n",
            "train: epoch 49, (4.267 secs), cent 0.014816, acc 0.995933\n",
            "test: epoch 49, (4.513 secs), cent 0.041896, acc 0.987700\n",
            "Epoch 49 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 50 start, learning rate 0.000100\n",
            "train: epoch 50, (4.257 secs), cent 0.014276, acc 0.996000\n",
            "test: epoch 50, (4.512 secs), cent 0.041620, acc 0.987800\n",
            "Epoch 50 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 51 start, learning rate 0.000100\n",
            "train: epoch 51, (4.357 secs), cent 0.014780, acc 0.995767\n",
            "test: epoch 51, (4.609 secs), cent 0.041467, acc 0.987800\n",
            "Epoch 51 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 52 start, learning rate 0.000100\n",
            "train: epoch 52, (4.348 secs), cent 0.015164, acc 0.995350\n",
            "test: epoch 52, (4.593 secs), cent 0.041574, acc 0.988000\n",
            "Epoch 52 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 53 start, learning rate 0.000100\n",
            "train: epoch 53, (4.281 secs), cent 0.014415, acc 0.996100\n",
            "test: epoch 53, (4.529 secs), cent 0.041573, acc 0.988000\n",
            "Epoch 53 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 54 start, learning rate 0.000100\n",
            "train: epoch 54, (4.100 secs), cent 0.014957, acc 0.995617\n",
            "test: epoch 54, (4.332 secs), cent 0.041790, acc 0.988100\n",
            "Epoch 54 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 55 start, learning rate 0.000100\n",
            "train: epoch 55, (4.154 secs), cent 0.014503, acc 0.995767\n",
            "test: epoch 55, (4.398 secs), cent 0.041982, acc 0.987800\n",
            "Epoch 55 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 56 start, learning rate 0.000100\n",
            "train: epoch 56, (4.245 secs), cent 0.014843, acc 0.996017\n",
            "test: epoch 56, (4.481 secs), cent 0.041735, acc 0.987800\n",
            "Epoch 56 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 57 start, learning rate 0.000100\n",
            "train: epoch 57, (4.177 secs), cent 0.014728, acc 0.995800\n",
            "test: epoch 57, (4.419 secs), cent 0.041808, acc 0.987700\n",
            "Epoch 57 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 58 start, learning rate 0.000100\n",
            "train: epoch 58, (4.121 secs), cent 0.014840, acc 0.995817\n",
            "test: epoch 58, (4.357 secs), cent 0.041724, acc 0.987500\n",
            "Epoch 58 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 59 start, learning rate 0.000100\n",
            "train: epoch 59, (4.058 secs), cent 0.014415, acc 0.995833\n",
            "test: epoch 59, (4.296 secs), cent 0.041864, acc 0.987900\n",
            "Epoch 59 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n",
            "Epoch 60 start, learning rate 0.000100\n",
            "train: epoch 60, (4.059 secs), cent 0.014104, acc 0.996083\n",
            "test: epoch 60, (4.295 secs), cent 0.041675, acc 0.987700\n",
            "Epoch 60 start, learning rate 0.000100\n",
            "n_active: [367, 199, 152]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O72sZ36dIosC",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYDT-eQ1kwQz",
        "colab_type": "code",
        "outputId": "7dbdb5ad-1b6f-4380-dd13-804e895ee9b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "def test():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "    logger = Accumulator('cent', 'acc')\n",
        "    to_run = [tnet['cent'], tnet['acc']]\n",
        "    for j in range(n_test_batches):\n",
        "        bx, by = mnist.test.next_batch(batch_size)\n",
        "        logger.accum(sess.run(to_run, {x:bx, y:by}))\n",
        "    logger.print_(header='test')\n",
        "    \n",
        "    n_active = sess.run(tnet['n_active'])\n",
        "    print(\"The percentage of activated neurons per layer:\")\n",
        "    for na, nl in zip(n_active, [784, 500, 300]):\n",
        "      print('{}/{} = {:.2f}%'.format(na, nl, float(na)/nl * 100))\n",
        "    \n",
        "test()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test: cent 0.041675, acc 0.987700\n",
            "The percentage of activated neurons per layer:\n",
            "367/784 = 46.81%\n",
            "199/500 = 39.80%\n",
            "152/300 = 50.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8y4uwf-IqiR",
        "colab_type": "text"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0xBULDNoNG_",
        "colab_type": "code",
        "outputId": "9f417b0e-23bb-4fe4-d7a8-3237648a4fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "def visualize():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    n_drop = len(tnet['n_active'])\n",
        "    fig = figure('pi', figsize=(8,6))\n",
        "    axarr = fig.subplots(n_drop)\n",
        "    for i in range(n_drop):\n",
        "        np_pi = sess.run(tnet['pi'][i]).reshape((1,-1))\n",
        "        im = axarr[i].imshow(np_pi, cmap='Blues', aspect='auto')\n",
        "        axarr[i].yaxis.set_visible(False)\n",
        "        axarr[i].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        if i == n_drop-1:\n",
        "            axarr[i].set_xlabel('The Number of Neurons\\nLeNet [784, 500, 300]')\n",
        "        fig.colorbar(im, ax=axarr[i])\n",
        "    show()\n",
        "visualize()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGCCAYAAABaRzyuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x432 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOcZDWwRVyN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}