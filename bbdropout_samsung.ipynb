{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bbdropout_samsung",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayeonLee/sparsification_samsung/blob/master/bbdropout_samsung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD6bQj0iVZM9",
        "colab_type": "text"
      },
      "source": [
        "# Network Sparsification Example : Beta-Bernoulli Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSc0p6Y6VvoL",
        "colab_type": "text"
      },
      "source": [
        "[Adaptive Network Sparsification with Dependent Variational Beta-Bernoulli Dropout](https://arxiv.org/abs/1805.10896)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP0DgMlDV64u",
        "colab_type": "text"
      },
      "source": [
        "## Import Tensorflow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNpLz5neUrqd",
        "colab_type": "code",
        "outputId": "9c7d6f50-8a64-40e5-8885-01b2d6001873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "source": [
        "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#기존에 설치된 다른 버전의 tensorflow를 제거합니다.\n",
        "!pip uninstall tensorboard -y\n",
        "!pip uninstall tensorflow-gpu -y\n",
        "!pip uninstall tensorflow -y\n",
        "#tensorflow gpu 버전을 설치합니다\n",
        "!pip install tensorflow-gpu==1.14"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorboard-1.14.0:\n",
            "  Successfully uninstalled tensorboard-1.14.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "Uninstalling tensorflow-1.14.0:\n",
            "  Successfully uninstalled tensorflow-1.14.0\n",
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/6d/2348df00a34baaabdef0fdb4f46f962f7a8a6720362c26c3a44a249767ea/tensorflow_gpu-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 49kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.33.4)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow-gpu==1.14)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/37/e6a7af1c92c5b68fb427f853b06164b56ea92126bcfd87784334ec5e4d42/tensorboard-1.14.0-py2-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.1.7)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.0.post1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==1.14) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.14) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.14) (5.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n",
            "Installing collected packages: tensorboard, tensorflow-gpu\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68oyuhXNWEQ6",
        "colab_type": "code",
        "outputId": "5bddc569-37bb-4a88-c5d8-d509c3c2d3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf # tensorflow를 import해줍니다\n",
        "tf.__version__ # 내가 사용할 tensorflow의 버전을 나타냅니다"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkYfIMpyYdPZ",
        "colab_type": "code",
        "outputId": "26a07703-374d-4b8d-ac49-1effbcb58351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "# pretrain된 lenet의 체크포인트 파일을 가져옵니다.\n",
        "!mkdir -p results/\n",
        "!wget -O lenet_dense_pretrained.zip https://github.com/HayeonLee/sparsification_samsung/blob/master/lenet_dense_pretrained.zip?raw=true\n",
        "!unzip lenet_dense_pretrained.zip -d results/\n",
        "!rm lenet_dense_pretrained.zip\n",
        "!ls\n",
        "!ls results/pretrained/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-23 05:33:12--  https://github.com/HayeonLee/sparsification_samsung/blob/master/lenet_dense_pretrained.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/HayeonLee/sparsification_samsung/raw/master/lenet_dense_pretrained.zip [following]\n",
            "--2019-07-23 05:33:12--  https://github.com/HayeonLee/sparsification_samsung/raw/master/lenet_dense_pretrained.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/HayeonLee/sparsification_samsung/master/lenet_dense_pretrained.zip [following]\n",
            "--2019-07-23 05:33:12--  https://raw.githubusercontent.com/HayeonLee/sparsification_samsung/master/lenet_dense_pretrained.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2080208 (2.0M) [application/zip]\n",
            "Saving to: ‘lenet_dense_pretrained.zip’\n",
            "\n",
            "lenet_dense_pretrai 100%[===================>]   1.98M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2019-07-23 05:33:13 (27.5 MB/s) - ‘lenet_dense_pretrained.zip’ saved [2080208/2080208]\n",
            "\n",
            "Archive:  lenet_dense_pretrained.zip\n",
            "  inflating: results/pretrained/checkpoint  \n",
            "  inflating: results/pretrained/model.data-00000-of-00001  \n",
            "  inflating: results/pretrained/model.index  \n",
            "  inflating: results/pretrained/model.meta  \n",
            "results  sample_data\n",
            "checkpoint  model.data-00000-of-00001  model.index  model.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR4m7OXGAt8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 라이브러리를 임포트합니다.\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import os\n",
        "from pylab import *\n",
        "import numpy as np\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.contrib.distributions import RelaxedBernoulli\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ0R62L8WvSu",
        "colab_type": "text"
      },
      "source": [
        "## Define the functions and utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFCAzQYqfrzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph() # 기존의 그려진 텐서플로우 그래프를 제거합니다.\n",
        "# 자주 쓰는 텐서플로우 함수의 약어를 지정합니다.\n",
        "logit = lambda x: tf.log(x + 1e-20) - tf.log(1-x + 1e-20)\n",
        "softplus = tf.nn.softplus\n",
        "relu = tf.nn.relu\n",
        "\n",
        "dense = tf.layers.dense\n",
        "flatten = tf.contrib.layers.flatten\n",
        "\n",
        "def conv(x, filters, kernel_size=3, strides=1, **kwargs):\n",
        "    return tf.layers.conv2d(x, filters, kernel_size, strides,\n",
        "            data_format='channels_first', **kwargs)\n",
        "\n",
        "def pool(x, **kwargs):\n",
        "    return tf.layers.max_pooling2d(x, 2, 2,\n",
        "            data_format='channels_first', **kwargs)\n",
        "\n",
        "def global_avg_pool(x):\n",
        "    return tf.reduce_mean(x, axis=[2, 3])\n",
        "\n",
        "layer_norm = tf.contrib.layers.layer_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJrRN4ujgQh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils/train.py\n",
        "# 필요한 함수를 정의합니다.\n",
        "def cross_entropy(logits, labels):\n",
        "    return tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels)\n",
        "\n",
        "def weight_decay(decay, var_list=None):\n",
        "    var_list = tf.trainable_variables() if var_list is None else var_list\n",
        "    return decay*tf.add_n([tf.nn.l2_loss(var) for var in var_list])\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "  \n",
        "def digamma_approx(x):\n",
        "# @MISC {1446110,\n",
        "# TITLE = {Approximating the Digamma function},\n",
        "# AUTHOR = {njuffa (https://math.stackexchange.com/users/114200/njuffa)},\n",
        "# HOWPUBLISHED = {Mathematics Stack Exchange},\n",
        "# NOTE = {URL:https://math.stackexchange.com/q/1446110 (version: 2015-09-22)},\n",
        "# EPRINT = {https://math.stackexchange.com/q/1446110},\n",
        "# URL = {https://math.stackexchange.com/q/1446110}}\n",
        "    def digamma_over_one(x):\n",
        "        return tf.log(x + 0.4849142940227510) \\\n",
        "                - 1/(1.0271785180163817*x)\n",
        "    return digamma_over_one(x+1) - 1./x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9RShCp_DDOz",
        "colab": {}
      },
      "source": [
        "# log를 출력하기 위한 함수를 선언합니다.\n",
        "class Accumulator():\n",
        "    def __init__(self, *args):\n",
        "        self.args = args\n",
        "        self.argdict = {}\n",
        "        for i, arg in enumerate(args):\n",
        "            self.argdict[arg] = i\n",
        "        self.sums = [0]*len(args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def accum(self, val):\n",
        "        val = [val] if type(val) is not list else val\n",
        "        val = [v for v in val if v is not None]\n",
        "        assert(len(val) == len(self.args))\n",
        "        for i in range(len(val)):\n",
        "            self.sums[i] += val[i]\n",
        "        self.cnt += 1\n",
        "\n",
        "    def clear(self):\n",
        "        self.sums = [0]*len(self.args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def get(self, arg, avg=True):\n",
        "        i = self.argdict.get(arg, -1)\n",
        "        assert(i is not -1)\n",
        "        return (self.sums[i]/self.cnt if avg else self.sums[i])\n",
        "\n",
        "    def print_(self, header=None, epoch=None, it=None, time=None,\n",
        "            logfile=None, do_not_print=[], as_int=[],\n",
        "            avg=True):\n",
        "        line = '' if header is None else header + ': '\n",
        "        if epoch is not None:\n",
        "            line += ('epoch %d, ' % epoch)\n",
        "        if it is not None:\n",
        "            line += ('iter %d, ' % it)\n",
        "        if time is not None:\n",
        "            line += ('(%.3f secs), ' % time)\n",
        "\n",
        "        args = [arg for arg in self.args if arg not in do_not_print]\n",
        "\n",
        "        for arg in args[:-1]:\n",
        "            val = self.sums[self.argdict[arg]]\n",
        "            if avg:\n",
        "                val /= self.cnt\n",
        "            if arg in as_int:\n",
        "                line += ('%s %d, ' % (arg, int(val)))\n",
        "            else:\n",
        "                line += ('%s %f, ' % (arg, val))\n",
        "        val = self.sums[self.argdict[args[-1]]]\n",
        "        if avg:\n",
        "            val /= self.cnt\n",
        "        if arg in as_int:\n",
        "            line += ('%s %d, ' % (arg, int(val)))\n",
        "        else:\n",
        "            line += ('%s %f' % (args[-1], val))\n",
        "        print(line)\n",
        "\n",
        "        if logfile is not None:\n",
        "            logfile.write(line + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF1WUmZRIdss",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the dataset: MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q21ZNy8bhTpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MNIST_PATH = './mnist'\n",
        "\n",
        "def mnist_input(batch_size):\n",
        "    mnist = input_data.read_data_sets(MNIST_PATH, one_hot=True, validation_size=0)\n",
        "    n_train_batches = mnist.train.num_examples/batch_size\n",
        "    n_test_batches = mnist.test.num_examples/batch_size\n",
        "    return mnist, n_train_batches, n_test_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3lKC6JjHRKM",
        "colab_type": "text"
      },
      "source": [
        "##Create models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg7YnEexflxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fully connected layers로 구성된 lenet을 선언합니다. \n",
        "def lenet_dense(x, y, training, name='lenet', reuse=None,\n",
        "        dropout=None, **dropout_kwargs):\n",
        "    dropout_ = lambda x, subname: x if dropout is None else \\\n",
        "            dropout(x, training, name=name+subname, reuse=reuse,\n",
        "                    **dropout_kwargs)\n",
        "    x = dense(dropout_(x, '/dropout1'), 500, activation=relu,\n",
        "            name=name+'/dense1', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout2'), 300, activation=relu,\n",
        "            name=name+'/dense2', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout3'), 10, name=name+'/dense3', reuse=reuse)\n",
        "\n",
        "    net = {}\n",
        "    all_vars = tf.get_collection('variables', scope=name)\n",
        "    net['qpi_vars'] = [v for v in all_vars if 'qpi_vars' in v.name]\n",
        "    net['pzx_vars'] = [v for v in all_vars if 'pzx_vars' in v.name]\n",
        "    net['weights'] = [v for v in all_vars \\\n",
        "            if 'qpi_vars' not in v.name and 'pzx_vars' not in v.name]\n",
        "\n",
        "    net['cent'] = cross_entropy(x, y)\n",
        "    net['wd'] = weight_decay(1e-4, var_list=net['weights'])\n",
        "    net['acc'] = accuracy(x, y)\n",
        "\n",
        "    prefix = 'train_' if training else 'test_'\n",
        "    net['kl'] = tf.get_collection('kl')\n",
        "    net['pi'] = tf.get_collection(prefix+'pi')\n",
        "    net['n_active'] = tf.get_collection(prefix+'n_active')\n",
        "\n",
        "    return net\n",
        "\n",
        "# def lenet_conv(x, y, training, name='lenet', reuse=None,\n",
        "#         dropout=None, **dropout_kwargs):\n",
        "#     dropout_ = lambda x, subname: x if dropout is None else \\\n",
        "#             dropout(x, training, name=name+subname, reuse=reuse,\n",
        "#                     **dropout_kwargs)\n",
        "#     x = tf.reshape(x, [-1, 1, 28, 28])\n",
        "#     x = conv(x, 20, 5, name=name+'/conv1', reuse=reuse)\n",
        "#     x = relu(dropout_(x, '/dropout1'))\n",
        "#     x = pool(x, name=name+'/pool1')\n",
        "#     x = conv(x, 50, 5, name=name+'/conv2', reuse=reuse)\n",
        "#     x = relu(dropout_(x, '/dropout2'))\n",
        "#     x = pool(x, name=name+'/pool2')\n",
        "#     x = flatten(x)\n",
        "#     x = dense(dropout_(x, '/dropout3'), 500, activation=relu,\n",
        "#             name=name+'/dense1', reuse=reuse)\n",
        "#     x = dense(dropout_(x, '/dropout4'), 10, name=name+'/dense2', reuse=reuse)\n",
        "\n",
        "#     net = {}\n",
        "#     all_vars = tf.get_collection('variables', scope=name)\n",
        "#     net['qpi_vars'] = [v for v in all_vars if 'qpi_vars' in v.name]\n",
        "#     net['pzx_vars'] = [v for v in all_vars if 'pzx_vars' in v.name]\n",
        "#     net['weights'] = [v for v in all_vars \\\n",
        "#             if 'qpi_vars' not in v.name and 'pzx_vars' not in v.name]\n",
        "\n",
        "#     net['cent'] = cross_entropy(x, y)\n",
        "#     net['wd'] = weight_decay(1e-4, var_list=net['weights'])\n",
        "#     net['acc'] = accuracy(x, y)\n",
        "\n",
        "#     prefix = 'train_' if training else 'test_'\n",
        "#     net['kl'] = tf.get_collection('kl')\n",
        "#     net['pi'] = tf.get_collection(prefix+'pi')\n",
        "#     net['n_active'] = tf.get_collection(prefix+'n_active')\n",
        "\n",
        "#     return net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA7g-bi7IP6w",
        "colab_type": "text"
      },
      "source": [
        "## Define the Beta-Bernoulli Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJjoi_-lga15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgamma = tf.lgamma\n",
        "Euler = 0.577215664901532\n",
        "\n",
        "def bbdropout(x, training,\n",
        "        alpha=1e-4, thres=1e-2, a_init=-1., tau=1e-1, center_init=1.0,\n",
        "        approx_digamma=True, scale_kl=None, dep=False,\n",
        "        unit_scale=True, collect=True,\n",
        "        name='bbdropout', reuse=None):\n",
        "\n",
        "    N = tf.shape(x)[0]\n",
        "    K = x.shape[1].value\n",
        "    is_conv = len(x.shape)==4\n",
        "    log = lambda x: tf.log(x + 1e-20)\n",
        "\n",
        "    with tf.variable_scope(name+'/qpi_vars', reuse=reuse):\n",
        "        with tf.device('/cpu:0'):\n",
        "            a = softplus(tf.get_variable('a_uc', shape=[K],\n",
        "                initializer=tf.constant_initializer(a_init)))\n",
        "            b = softplus(tf.get_variable('b_uc', shape=[K]))\n",
        "\n",
        "    _digamma = digamma_approx \n",
        "    kl = (a-alpha)/a * (-Euler - _digamma(b) - 1/b) \\\n",
        "            + log(a*b) - log(alpha) - (b-1)/b\n",
        "    pi = (1 - tf.random_uniform([K])**(1/b))**(1/a) if training else \\\n",
        "            b*tf.exp(lgamma(1+1/a) + lgamma(b) - lgamma(1+1/a+b))\n",
        "    \n",
        "    if training:\n",
        "        z = RelaxedBernoulli(tau, logits=logit(pi)).sample(N)\n",
        "    else:\n",
        "        pi_ = tf.where(tf.greater(pi, thres), pi, tf.zeros_like(pi))\n",
        "        z = tf.tile(tf.expand_dims(pi_, 0), [N, 1])\n",
        "    n_active = tf.reduce_sum(tf.cast(tf.greater(pi, thres), tf.int32))\n",
        "\n",
        "    if scale_kl is None:\n",
        "        kl = tf.reduce_sum(kl)\n",
        "    else:\n",
        "        kl = scale_kl * tf.reduce_mean(kl)\n",
        "\n",
        "    if collect:\n",
        "        if reuse is not True:\n",
        "            tf.add_to_collection('kl', kl)\n",
        "        prefix = 'train_' if training else 'test_'\n",
        "        tf.add_to_collection(prefix+'pi', pi)\n",
        "        tf.add_to_collection(prefix+'n_active', n_active)\n",
        "\n",
        "    z = tf.reshape(z, ([-1, K, 1, 1] if is_conv else [-1, K]))\n",
        "    return x*z\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InEG2TA3Iilj",
        "colab_type": "text"
      },
      "source": [
        "## Let's run the code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz6djPqUWps9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretraindir = './results/pretrained' \n",
        "savedir = './results/bbdropout/sample_run' \n",
        "if not os.path.isdir(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "batch_size = 100\n",
        "n_epochs = 60\n",
        "save_freq = 20\n",
        "mnist, n_train_batches, n_test_batches = mnist_input(batch_size)\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "N = mnist.train.num_examples\n",
        "dropout = bbdropout\n",
        "net = lenet_dense(x, y, True, dropout=dropout)\n",
        "tnet = lenet_dense(x, y, False, reuse=True, dropout=dropout)\n",
        "\n",
        "def train():\n",
        "    loss = net['cent'] + tf.add_n(net['kl'])/float(N) + net['wd'] \n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    bdr = [int(n_train_batches*(n_epochs-1)*r) for r in [0.5, 0.75]]\n",
        "    vals = [1e-2, 1e-3, 1e-4]\n",
        "    lr = tf.train.piecewise_constant(tf.cast(global_step, tf.int32), bdr, vals)\n",
        "    train_op1 = tf.train.AdamOptimizer(lr).minimize(loss,\n",
        "            var_list=net['qpi_vars'], global_step=global_step)\n",
        "    train_op2 = tf.train.AdamOptimizer(0.1*lr).minimize(loss,\n",
        "            var_list=net['weights'])\n",
        "    train_op = tf.group(train_op1, train_op2)\n",
        "\n",
        "    pretrain_saver = tf.train.Saver(net['weights'])\n",
        "    saver = tf.train.Saver(net['weights']+net['qpi_vars'])\n",
        "    logfile = open(os.path.join(savedir, 'train.log'), 'w', 0)\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    pretrain_saver.restore(sess, os.path.join(pretraindir, 'model'))\n",
        "\n",
        "    train_logger = Accumulator('cent', 'acc')\n",
        "    train_to_run = [train_op, net['cent'], net['acc']]\n",
        "    test_logger = Accumulator('cent', 'acc')\n",
        "    test_to_run = [tnet['cent'], tnet['acc']]\n",
        "    for i in range(n_epochs):\n",
        "        line = 'Epoch %d start, learning rate %f' % (i+1, sess.run(lr))\n",
        "        print(line)\n",
        "        logfile.write(line + '\\n')\n",
        "        train_logger.clear()\n",
        "        start = time.time()\n",
        "        for j in range(n_train_batches):\n",
        "            bx, by = mnist.train.next_batch(batch_size)\n",
        "            train_logger.accum(sess.run(train_to_run, {x:bx, y:by}))\n",
        "        train_logger.print_(header='train', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "\n",
        "        test_logger.clear()\n",
        "        for j in range(n_test_batches):\n",
        "            bx, by = mnist.test.next_batch(batch_size)\n",
        "            test_logger.accum(sess.run(test_to_run, {x:bx, y:by}))\n",
        "        test_logger.print_(header='test', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "        #line = 'kl: ' + str(sess.run(tnet['kl'])) + '\\n'\n",
        "        line += 'n_active: ' + str(sess.run(tnet['n_active'])) + '\\n'\n",
        "        print(line)\n",
        "        logfile.write(line+'\\n')\n",
        "\n",
        "        if (i+1)% save_freq == 0:\n",
        "            saver.save(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    logfile.close()\n",
        "    saver.save(sess, os.path.join(savedir, 'model'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_0exUrqhel8",
        "colab_type": "code",
        "outputId": "241305de-cafe-47e3-d79e-7aa8ee53a462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0723 05:34:16.192369 139630325450624 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 start, learning rate 0.010000\n",
            "train: epoch 1, (5.304 secs), cent 0.413326, acc 0.877817\n",
            "test: epoch 1, (5.596 secs), cent 0.110209, acc 0.967600\n",
            "kl: [4585.8677, 3876.8252, 2495.3503]\n",
            "n_active: [523, 469, 297]\n",
            "\n",
            "Epoch 2 start, learning rate 0.010000\n",
            "train: epoch 2, (4.134 secs), cent 0.164370, acc 0.949483\n",
            "test: epoch 2, (4.369 secs), cent 0.083183, acc 0.973700\n",
            "kl: [4452.902, 3971.1667, 2566.4827]\n",
            "n_active: [511, 467, 297]\n",
            "\n",
            "Epoch 3 start, learning rate 0.010000\n",
            "train: epoch 3, (4.131 secs), cent 0.125859, acc 0.960067\n",
            "test: epoch 3, (4.362 secs), cent 0.067286, acc 0.978200\n",
            "kl: [4552.4395, 4077.387, 2617.6377]\n",
            "n_active: [503, 465, 296]\n",
            "\n",
            "Epoch 4 start, learning rate 0.010000\n",
            "train: epoch 4, (4.080 secs), cent 0.101211, acc 0.968267\n",
            "test: epoch 4, (4.315 secs), cent 0.061414, acc 0.980100\n",
            "kl: [4632.4185, 4125.4053, 2639.4167]\n",
            "n_active: [499, 463, 296]\n",
            "\n",
            "Epoch 5 start, learning rate 0.010000\n",
            "train: epoch 5, (4.065 secs), cent 0.087104, acc 0.972150\n",
            "test: epoch 5, (4.302 secs), cent 0.057151, acc 0.982500\n",
            "kl: [4654.086, 4140.8936, 2655.01]\n",
            "n_active: [495, 458, 296]\n",
            "\n",
            "Epoch 6 start, learning rate 0.010000\n",
            "train: epoch 6, (4.114 secs), cent 0.078007, acc 0.975200\n",
            "test: epoch 6, (4.347 secs), cent 0.060103, acc 0.980900\n",
            "kl: [4659.2407, 4163.241, 2645.6758]\n",
            "n_active: [491, 457, 296]\n",
            "\n",
            "Epoch 7 start, learning rate 0.010000\n",
            "train: epoch 7, (4.050 secs), cent 0.072933, acc 0.976917\n",
            "test: epoch 7, (4.301 secs), cent 0.052106, acc 0.984000\n",
            "kl: [4662.164, 4165.1523, 2661.3926]\n",
            "n_active: [486, 455, 296]\n",
            "\n",
            "Epoch 8 start, learning rate 0.010000\n",
            "train: epoch 8, (4.076 secs), cent 0.065090, acc 0.978933\n",
            "test: epoch 8, (4.309 secs), cent 0.058208, acc 0.982000\n",
            "kl: [4661.3584, 4135.7266, 2656.5105]\n",
            "n_active: [481, 449, 296]\n",
            "\n",
            "Epoch 9 start, learning rate 0.010000\n",
            "train: epoch 9, (4.098 secs), cent 0.063245, acc 0.979617\n",
            "test: epoch 9, (4.334 secs), cent 0.052368, acc 0.983900\n",
            "kl: [4619.59, 4074.6233, 2647.5076]\n",
            "n_active: [475, 442, 296]\n",
            "\n",
            "Epoch 10 start, learning rate 0.010000\n",
            "train: epoch 10, (4.039 secs), cent 0.060040, acc 0.980317\n",
            "test: epoch 10, (4.278 secs), cent 0.050812, acc 0.983900\n",
            "kl: [4588.412, 4009.9785, 2640.6538]\n",
            "n_active: [469, 437, 296]\n",
            "\n",
            "Epoch 11 start, learning rate 0.010000\n",
            "train: epoch 11, (4.063 secs), cent 0.056264, acc 0.982183\n",
            "test: epoch 11, (4.305 secs), cent 0.054078, acc 0.982300\n",
            "kl: [4527.2466, 3933.9995, 2614.82]\n",
            "n_active: [465, 427, 296]\n",
            "\n",
            "Epoch 12 start, learning rate 0.010000\n",
            "train: epoch 12, (4.029 secs), cent 0.054491, acc 0.982250\n",
            "test: epoch 12, (4.268 secs), cent 0.054052, acc 0.982800\n",
            "kl: [4489.118, 3859.9668, 2600.5745]\n",
            "n_active: [457, 415, 296]\n",
            "\n",
            "Epoch 13 start, learning rate 0.010000\n",
            "train: epoch 13, (4.067 secs), cent 0.049668, acc 0.983483\n",
            "test: epoch 13, (4.308 secs), cent 0.053748, acc 0.983000\n",
            "kl: [4432.0723, 3751.5767, 2587.7563]\n",
            "n_active: [452, 387, 294]\n",
            "\n",
            "Epoch 14 start, learning rate 0.010000\n",
            "train: epoch 14, (4.090 secs), cent 0.049332, acc 0.983783\n",
            "test: epoch 14, (4.325 secs), cent 0.053274, acc 0.983200\n",
            "kl: [4400.6597, 3631.7056, 2557.7808]\n",
            "n_active: [447, 374, 289]\n",
            "\n",
            "Epoch 15 start, learning rate 0.010000\n",
            "train: epoch 15, (4.092 secs), cent 0.048496, acc 0.984083\n",
            "test: epoch 15, (4.324 secs), cent 0.051268, acc 0.985100\n",
            "kl: [4352.9336, 3525.1978, 2529.8875]\n",
            "n_active: [442, 356, 286]\n",
            "\n",
            "Epoch 16 start, learning rate 0.010000\n",
            "train: epoch 16, (4.145 secs), cent 0.046219, acc 0.985017\n",
            "test: epoch 16, (4.392 secs), cent 0.053188, acc 0.984300\n",
            "kl: [4314.6836, 3407.0432, 2498.8728]\n",
            "n_active: [437, 336, 282]\n",
            "\n",
            "Epoch 17 start, learning rate 0.010000\n",
            "train: epoch 17, (4.096 secs), cent 0.042273, acc 0.986217\n",
            "test: epoch 17, (4.330 secs), cent 0.050583, acc 0.983700\n",
            "kl: [4266.914, 3272.0469, 2453.1057]\n",
            "n_active: [431, 324, 278]\n",
            "\n",
            "Epoch 18 start, learning rate 0.010000\n",
            "train: epoch 18, (4.119 secs), cent 0.042527, acc 0.985533\n",
            "test: epoch 18, (4.360 secs), cent 0.052810, acc 0.983800\n",
            "kl: [4230.395, 3147.3342, 2393.1646]\n",
            "n_active: [426, 308, 265]\n",
            "\n",
            "Epoch 19 start, learning rate 0.010000\n",
            "train: epoch 19, (4.179 secs), cent 0.041222, acc 0.986800\n",
            "test: epoch 19, (4.431 secs), cent 0.051536, acc 0.984800\n",
            "kl: [4168.8467, 3056.9133, 2330.7346]\n",
            "n_active: [417, 298, 259]\n",
            "\n",
            "Epoch 20 start, learning rate 0.010000\n",
            "train: epoch 20, (4.189 secs), cent 0.041667, acc 0.986483\n",
            "test: epoch 20, (4.422 secs), cent 0.056277, acc 0.983500\n",
            "kl: [4106.506, 2963.5393, 2272.0024]\n",
            "n_active: [413, 286, 256]\n",
            "\n",
            "Epoch 21 start, learning rate 0.010000\n",
            "train: epoch 21, (4.124 secs), cent 0.038673, acc 0.987333\n",
            "test: epoch 21, (4.365 secs), cent 0.053269, acc 0.985200\n",
            "kl: [4068.9468, 2878.0254, 2212.1665]\n",
            "n_active: [404, 278, 248]\n",
            "\n",
            "Epoch 22 start, learning rate 0.010000\n",
            "train: epoch 22, (4.166 secs), cent 0.037464, acc 0.987933\n",
            "test: epoch 22, (4.410 secs), cent 0.052028, acc 0.984300\n",
            "kl: [4006.4717, 2784.0107, 2158.2024]\n",
            "n_active: [400, 265, 241]\n",
            "\n",
            "Epoch 23 start, learning rate 0.010000\n",
            "train: epoch 23, (4.109 secs), cent 0.039050, acc 0.986700\n",
            "test: epoch 23, (4.345 secs), cent 0.057510, acc 0.982600\n",
            "kl: [3950.9033, 2714.4941, 2098.8716]\n",
            "n_active: [392, 257, 233]\n",
            "\n",
            "Epoch 24 start, learning rate 0.010000\n",
            "train: epoch 24, (4.098 secs), cent 0.036773, acc 0.987850\n",
            "test: epoch 24, (4.332 secs), cent 0.053785, acc 0.983300\n",
            "kl: [3920.2754, 2652.431, 2044.3187]\n",
            "n_active: [385, 248, 220]\n",
            "\n",
            "Epoch 25 start, learning rate 0.010000\n",
            "train: epoch 25, (4.096 secs), cent 0.036091, acc 0.988417\n",
            "test: epoch 25, (4.324 secs), cent 0.052029, acc 0.984500\n",
            "kl: [3883.8364, 2576.942, 1983.7449]\n",
            "n_active: [382, 242, 211]\n",
            "\n",
            "Epoch 26 start, learning rate 0.010000\n",
            "train: epoch 26, (4.069 secs), cent 0.034700, acc 0.988883\n",
            "test: epoch 26, (4.318 secs), cent 0.053366, acc 0.984200\n",
            "kl: [3837.95, 2520.7158, 1920.3757]\n",
            "n_active: [381, 237, 205]\n",
            "\n",
            "Epoch 27 start, learning rate 0.010000\n",
            "train: epoch 27, (4.140 secs), cent 0.032078, acc 0.989383\n",
            "test: epoch 27, (4.376 secs), cent 0.053176, acc 0.983900\n",
            "kl: [3785.9, 2470.743, 1858.3224]\n",
            "n_active: [377, 232, 200]\n",
            "\n",
            "Epoch 28 start, learning rate 0.010000\n",
            "train: epoch 28, (4.087 secs), cent 0.034511, acc 0.988250\n",
            "test: epoch 28, (4.327 secs), cent 0.050675, acc 0.985900\n",
            "kl: [3745.2178, 2434.372, 1799.747]\n",
            "n_active: [377, 230, 192]\n",
            "\n",
            "Epoch 29 start, learning rate 0.010000\n",
            "train: epoch 29, (4.126 secs), cent 0.036333, acc 0.988117\n",
            "test: epoch 29, (4.370 secs), cent 0.054221, acc 0.983300\n",
            "kl: [3730.5554, 2411.1968, 1736.782]\n",
            "n_active: [374, 226, 178]\n",
            "\n",
            "Epoch 30 start, learning rate 0.010000\n",
            "train: epoch 30, (4.126 secs), cent 0.028972, acc 0.990700\n",
            "test: epoch 30, (4.370 secs), cent 0.044533, acc 0.985500\n",
            "kl: [3710.1802, 2387.4116, 1704.8694]\n",
            "n_active: [371, 222, 174]\n",
            "\n",
            "Epoch 31 start, learning rate 0.001000\n",
            "train: epoch 31, (4.105 secs), cent 0.019665, acc 0.994267\n",
            "test: epoch 31, (4.341 secs), cent 0.043309, acc 0.986600\n",
            "kl: [3704.041, 2379.04, 1697.4746]\n",
            "n_active: [371, 222, 173]\n",
            "\n",
            "Epoch 32 start, learning rate 0.001000\n",
            "train: epoch 32, (4.158 secs), cent 0.019003, acc 0.994017\n",
            "test: epoch 32, (4.386 secs), cent 0.042359, acc 0.986900\n",
            "kl: [3696.9673, 2370.8716, 1688.2783]\n",
            "n_active: [370, 222, 172]\n",
            "\n",
            "Epoch 33 start, learning rate 0.001000\n",
            "train: epoch 33, (4.142 secs), cent 0.017848, acc 0.994667\n",
            "test: epoch 33, (4.396 secs), cent 0.041521, acc 0.987300\n",
            "kl: [3687.9927, 2360.6045, 1676.9882]\n",
            "n_active: [369, 222, 172]\n",
            "\n",
            "Epoch 34 start, learning rate 0.001000\n",
            "train: epoch 34, (4.182 secs), cent 0.018162, acc 0.994183\n",
            "test: epoch 34, (4.415 secs), cent 0.042149, acc 0.986600\n",
            "kl: [3680.6768, 2350.8823, 1664.3634]\n",
            "n_active: [369, 222, 170]\n",
            "\n",
            "Epoch 35 start, learning rate 0.001000\n",
            "train: epoch 35, (4.112 secs), cent 0.016389, acc 0.995133\n",
            "test: epoch 35, (4.357 secs), cent 0.042511, acc 0.986800\n",
            "kl: [3670.918, 2339.8, 1650.3997]\n",
            "n_active: [367, 221, 169]\n",
            "\n",
            "Epoch 36 start, learning rate 0.001000\n",
            "train: epoch 36, (4.106 secs), cent 0.017400, acc 0.994617\n",
            "test: epoch 36, (4.342 secs), cent 0.041989, acc 0.987100\n",
            "kl: [3663.0752, 2326.6829, 1634.9849]\n",
            "n_active: [366, 218, 168]\n",
            "\n",
            "Epoch 37 start, learning rate 0.001000\n",
            "train: epoch 37, (4.101 secs), cent 0.017209, acc 0.994900\n",
            "test: epoch 37, (4.337 secs), cent 0.041410, acc 0.987100\n",
            "kl: [3655.028, 2314.181, 1619.0284]\n",
            "n_active: [364, 216, 165]\n",
            "\n",
            "Epoch 38 start, learning rate 0.001000\n",
            "train: epoch 38, (4.108 secs), cent 0.016654, acc 0.995017\n",
            "test: epoch 38, (4.345 secs), cent 0.040935, acc 0.987600\n",
            "kl: [3647.9146, 2301.991, 1602.3523]\n",
            "n_active: [364, 214, 165]\n",
            "\n",
            "Epoch 39 start, learning rate 0.001000\n",
            "train: epoch 39, (4.055 secs), cent 0.016898, acc 0.994917\n",
            "test: epoch 39, (4.289 secs), cent 0.041812, acc 0.987600\n",
            "kl: [3641.5667, 2288.9346, 1584.9062]\n",
            "n_active: [363, 214, 162]\n",
            "\n",
            "Epoch 40 start, learning rate 0.001000\n",
            "train: epoch 40, (4.242 secs), cent 0.016403, acc 0.994850\n",
            "test: epoch 40, (4.494 secs), cent 0.042552, acc 0.987200\n",
            "kl: [3633.3984, 2276.0269, 1569.0131]\n",
            "n_active: [363, 214, 160]\n",
            "\n",
            "Epoch 41 start, learning rate 0.001000\n",
            "train: epoch 41, (4.268 secs), cent 0.016558, acc 0.995117\n",
            "test: epoch 41, (4.513 secs), cent 0.041484, acc 0.986800\n",
            "kl: [3624.1567, 2264.7788, 1555.0383]\n",
            "n_active: [363, 213, 158]\n",
            "\n",
            "Epoch 42 start, learning rate 0.001000\n",
            "train: epoch 42, (4.161 secs), cent 0.016537, acc 0.995033\n",
            "test: epoch 42, (4.393 secs), cent 0.041491, acc 0.986900\n",
            "kl: [3616.5527, 2254.3909, 1541.5717]\n",
            "n_active: [363, 211, 157]\n",
            "\n",
            "Epoch 43 start, learning rate 0.001000\n",
            "train: epoch 43, (4.151 secs), cent 0.015944, acc 0.995483\n",
            "test: epoch 43, (4.395 secs), cent 0.041041, acc 0.987500\n",
            "kl: [3606.9062, 2244.9888, 1529.4296]\n",
            "n_active: [360, 210, 157]\n",
            "\n",
            "Epoch 44 start, learning rate 0.001000\n",
            "train: epoch 44, (4.100 secs), cent 0.015962, acc 0.995333\n",
            "test: epoch 44, (4.330 secs), cent 0.042071, acc 0.987800\n",
            "kl: [3599.6536, 2235.3599, 1518.4192]\n",
            "n_active: [360, 210, 157]\n",
            "\n",
            "Epoch 45 start, learning rate 0.001000\n",
            "train: epoch 45, (4.118 secs), cent 0.015535, acc 0.995400\n",
            "test: epoch 45, (4.348 secs), cent 0.041472, acc 0.987300\n",
            "kl: [3596.5962, 2231.8315, 1514.5891]\n",
            "n_active: [360, 210, 155]\n",
            "\n",
            "Epoch 46 start, learning rate 0.000100\n",
            "train: epoch 46, (4.102 secs), cent 0.015150, acc 0.995333\n",
            "test: epoch 46, (4.339 secs), cent 0.041450, acc 0.987200\n",
            "kl: [3595.483, 2230.9414, 1513.5421]\n",
            "n_active: [359, 210, 155]\n",
            "\n",
            "Epoch 47 start, learning rate 0.000100\n",
            "train: epoch 47, (4.073 secs), cent 0.015872, acc 0.995617\n",
            "test: epoch 47, (4.322 secs), cent 0.041560, acc 0.987200\n",
            "kl: [3594.5007, 2230.0571, 1512.3179]\n",
            "n_active: [358, 210, 155]\n",
            "\n",
            "Epoch 48 start, learning rate 0.000100\n",
            "train: epoch 48, (4.130 secs), cent 0.014674, acc 0.995950\n",
            "test: epoch 48, (4.367 secs), cent 0.041425, acc 0.987100\n",
            "kl: [3593.4143, 2229.1729, 1511.1366]\n",
            "n_active: [358, 210, 155]\n",
            "\n",
            "Epoch 49 start, learning rate 0.000100\n",
            "train: epoch 49, (4.145 secs), cent 0.014648, acc 0.996117\n",
            "test: epoch 49, (4.385 secs), cent 0.041257, acc 0.987200\n",
            "kl: [3592.237, 2228.19, 1509.8127]\n",
            "n_active: [358, 210, 154]\n",
            "\n",
            "Epoch 50 start, learning rate 0.000100\n",
            "train: epoch 50, (4.098 secs), cent 0.014464, acc 0.995933\n",
            "test: epoch 50, (4.333 secs), cent 0.041196, acc 0.987300\n",
            "kl: [3590.9985, 2227.189, 1508.5338]\n",
            "n_active: [358, 209, 154]\n",
            "\n",
            "Epoch 51 start, learning rate 0.000100\n",
            "train: epoch 51, (4.147 secs), cent 0.015881, acc 0.995550\n",
            "test: epoch 51, (4.385 secs), cent 0.041372, acc 0.987500\n",
            "kl: [3589.9463, 2226.1724, 1507.189]\n",
            "n_active: [358, 209, 153]\n",
            "\n",
            "Epoch 52 start, learning rate 0.000100\n",
            "train: epoch 52, (4.113 secs), cent 0.015503, acc 0.995633\n",
            "test: epoch 52, (4.348 secs), cent 0.041393, acc 0.987500\n",
            "kl: [3588.7607, 2225.2092, 1505.8618]\n",
            "n_active: [358, 209, 153]\n",
            "\n",
            "Epoch 53 start, learning rate 0.000100\n",
            "train: epoch 53, (4.113 secs), cent 0.014833, acc 0.995950\n",
            "test: epoch 53, (4.343 secs), cent 0.041198, acc 0.987400\n",
            "kl: [3587.5706, 2224.194, 1504.5676]\n",
            "n_active: [358, 209, 153]\n",
            "\n",
            "Epoch 54 start, learning rate 0.000100\n",
            "train: epoch 54, (4.073 secs), cent 0.014705, acc 0.995800\n",
            "test: epoch 54, (4.304 secs), cent 0.041337, acc 0.987700\n",
            "kl: [3586.5251, 2223.1682, 1503.0819]\n",
            "n_active: [358, 209, 153]\n",
            "\n",
            "Epoch 55 start, learning rate 0.000100\n",
            "train: epoch 55, (4.079 secs), cent 0.014393, acc 0.995650\n",
            "test: epoch 55, (4.312 secs), cent 0.041223, acc 0.987800\n",
            "kl: [3585.3748, 2222.126, 1501.7935]\n",
            "n_active: [358, 209, 152]\n",
            "\n",
            "Epoch 56 start, learning rate 0.000100\n",
            "train: epoch 56, (4.056 secs), cent 0.014494, acc 0.995883\n",
            "test: epoch 56, (4.297 secs), cent 0.041063, acc 0.987700\n",
            "kl: [3584.174, 2221.1023, 1500.5166]\n",
            "n_active: [358, 209, 152]\n",
            "\n",
            "Epoch 57 start, learning rate 0.000100\n",
            "train: epoch 57, (4.060 secs), cent 0.014619, acc 0.995817\n",
            "test: epoch 57, (4.291 secs), cent 0.041206, acc 0.987800\n",
            "kl: [3582.9202, 2220.1245, 1499.2695]\n",
            "n_active: [358, 209, 152]\n",
            "\n",
            "Epoch 58 start, learning rate 0.000100\n",
            "train: epoch 58, (4.106 secs), cent 0.014630, acc 0.995883\n",
            "test: epoch 58, (4.331 secs), cent 0.041172, acc 0.987900\n",
            "kl: [3581.6458, 2219.0532, 1497.9268]\n",
            "n_active: [358, 209, 152]\n",
            "\n",
            "Epoch 59 start, learning rate 0.000100\n",
            "train: epoch 59, (4.042 secs), cent 0.014999, acc 0.995383\n",
            "test: epoch 59, (4.281 secs), cent 0.041254, acc 0.987500\n",
            "kl: [3580.4988, 2218.096, 1496.7012]\n",
            "n_active: [358, 209, 152]\n",
            "\n",
            "Epoch 60 start, learning rate 0.000100\n",
            "train: epoch 60, (4.104 secs), cent 0.015115, acc 0.995767\n",
            "test: epoch 60, (4.334 secs), cent 0.041238, acc 0.987600\n",
            "kl: [3579.2612, 2217.1335, 1495.5554]\n",
            "n_active: [358, 209, 152]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O72sZ36dIosC",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYDT-eQ1kwQz",
        "colab_type": "code",
        "outputId": "aca4a87e-1b97-4216-dead-5f369c6bf9b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "def test():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "    logger = Accumulator('cent', 'acc')\n",
        "    to_run = [tnet['cent'], tnet['acc']]\n",
        "    for j in range(n_test_batches):\n",
        "        bx, by = mnist.test.next_batch(batch_size)\n",
        "        logger.accum(sess.run(to_run, {x:bx, y:by}))\n",
        "    logger.print_(header='test')\n",
        "    \n",
        "    n_active = sess.run(tnet['n_active'])\n",
        "    print(\"The percentage of activated neurons per layer:\")\n",
        "    for na, nl in zip(n_active, [784, 500, 300]):\n",
        "      print('{}/{} = {:.2f}%'.format(na, nl, float(na)/nl * 100))\n",
        "    \n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test: cent 0.041238, acc 0.987600\n",
            "The percentage of activated neurons per layer\n",
            "358/784 = 45.66%\n",
            "209/500 = 41.80%\n",
            "152/300 = 50.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8y4uwf-IqiR",
        "colab_type": "text"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0xBULDNoNG_",
        "colab_type": "code",
        "outputId": "f2e8d2bb-fd0b-4e6a-f59e-ac7c005d52d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "def visualize():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    n_drop = len(tnet['n_active'])\n",
        "    fig = figure('pi', figsize=(8,6))\n",
        "    axarr = fig.subplots(n_drop)\n",
        "    for i in range(n_drop):\n",
        "        np_pi = sess.run(tnet['pi'][i]).reshape((1,-1))\n",
        "        im = axarr[i].imshow(np_pi, cmap='Blues', aspect='auto')\n",
        "        axarr[i].yaxis.set_visible(False)\n",
        "        axarr[i].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        if i == n_drop-1:\n",
        "            axarr[i].set_xlabel('The Number of Neurons\\nLeNet [784, 500, 300]')\n",
        "        fig.colorbar(im, ax=axarr[i])\n",
        "    show()\n",
        "visualize()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGCCAYAAABaRzyuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhorSSpiIhorSSpiIhorV8n7A9Ngg7UWAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x432 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOcZDWwRVyN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}