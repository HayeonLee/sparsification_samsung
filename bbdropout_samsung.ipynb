{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bbdropout_samsung.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayeonLee/sparsification_samsung/blob/master/bbdropout_samsung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD6bQj0iVZM9",
        "colab_type": "text"
      },
      "source": [
        "# Network Sparsification Example : Beta-Bernoulli Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSc0p6Y6VvoL",
        "colab_type": "text"
      },
      "source": [
        "[Adaptive Network Sparsification with Dependent Variational Beta-Bernoulli Dropout](https://arxiv.org/abs/1805.10896)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP0DgMlDV64u",
        "colab_type": "text"
      },
      "source": [
        "### Import Tensorflow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNpLz5neUrqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "1fbe046c-157e-4ee9-ae8d-6d58dfb38065"
      },
      "source": [
        "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#기존에 설치된 다른 버전의 tensorflow를 제거합니다.\n",
        "!pip uninstall tensorboard -y\n",
        "!pip uninstall tensorflow-gpu -y\n",
        "!pip uninstall tensorflow -y\n",
        "#tensorflow gpu 버전을 설치합니다\n",
        "!pip install tensorflow-gpu==1.14"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorboard-1.14.0:\n",
            "  Successfully uninstalled tensorboard-1.14.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "Uninstalling tensorflow-1.14.0:\n",
            "  Successfully uninstalled tensorflow-1.14.0\n",
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/6d/2348df00a34baaabdef0fdb4f46f962f7a8a6720362c26c3a44a249767ea/tensorflow_gpu-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow-gpu==1.14)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/37/e6a7af1c92c5b68fb427f853b06164b56ea92126bcfd87784334ec5e4d42/tensorboard-1.14.0-py2-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.33.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.7.1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.6)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.1.7)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.0.post1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==1.14) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.14) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.14) (5.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n",
            "Installing collected packages: tensorboard, tensorflow-gpu\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68oyuhXNWEQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee17d5d0-64c3-45da-b1e8-50de51317f4e"
      },
      "source": [
        "import tensorflow as tf # tensorflow를 import해줍니다\n",
        "tf.__version__ # 내가 사용할 tensorflow의 버전을 나타냅니다"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkYfIMpyYdPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "37c3074c-189f-445c-c991-ff2e561f5f02"
      },
      "source": [
        "!mkdir -p results/\n",
        "!wget -O lenet_dense_pretrained.zip https://github.com/HayeonLee/sparsification_samsung/blob/master/lenet_dense_pretrained.zip?raw=true\n",
        "!unzip lenet_dense_pretrained.zip -d results/\n",
        "!rm lenet_dense_pretrained.zip\n",
        "!ls\n",
        "!ls results/pretrained/"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-21 12:49:04--  https://github.com/HayeonLee/sparsification_samsung/blob/master/lenet_dense_pretrained.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.118.4\n",
            "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/HayeonLee/sparsification_samsung/raw/master/lenet_dense_pretrained.zip [following]\n",
            "--2019-07-21 12:49:04--  https://github.com/HayeonLee/sparsification_samsung/raw/master/lenet_dense_pretrained.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/HayeonLee/sparsification_samsung/master/lenet_dense_pretrained.zip [following]\n",
            "--2019-07-21 12:49:05--  https://raw.githubusercontent.com/HayeonLee/sparsification_samsung/master/lenet_dense_pretrained.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2080208 (2.0M) [application/zip]\n",
            "Saving to: ‘lenet_dense_pretrained.zip’\n",
            "\n",
            "lenet_dense_pretrai 100%[===================>]   1.98M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-07-21 12:49:05 (50.4 MB/s) - ‘lenet_dense_pretrained.zip’ saved [2080208/2080208]\n",
            "\n",
            "Archive:  lenet_dense_pretrained.zip\n",
            "  inflating: results/pretrained/checkpoint  \n",
            "  inflating: results/pretrained/model.data-00000-of-00001  \n",
            "  inflating: results/pretrained/model.index  \n",
            "  inflating: results/pretrained/model.meta  \n",
            "data  mnist  results  sample_data\n",
            "checkpoint  model.data-00000-of-00001  model.index  model.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ0R62L8WvSu",
        "colab_type": "text"
      },
      "source": [
        "### Lenet Dense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFCAzQYqfrzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph() \n",
        "# layers.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "exp = tf.exp\n",
        "#log = lambda x: tf.log(x + 1e-20)\n",
        "logit = lambda x: tf.log(x + 1e-20) - tf.log(1-x + 1e-20)\n",
        "softplus = tf.nn.softplus\n",
        "softmax = tf.nn.softmax\n",
        "tanh = tf.nn.tanh\n",
        "relu = tf.nn.relu\n",
        "sigmoid = tf.nn.sigmoid\n",
        "\n",
        "dense = tf.layers.dense\n",
        "flatten = tf.contrib.layers.flatten\n",
        "\n",
        "def conv(x, filters, kernel_size=3, strides=1, **kwargs):\n",
        "    return tf.layers.conv2d(x, filters, kernel_size, strides,\n",
        "            data_format='channels_first', **kwargs)\n",
        "\n",
        "def pool(x, **kwargs):\n",
        "    return tf.layers.max_pooling2d(x, 2, 2,\n",
        "            data_format='channels_first', **kwargs)\n",
        "\n",
        "def global_avg_pool(x):\n",
        "    return tf.reduce_mean(x, axis=[2, 3])\n",
        "\n",
        "batch_norm = tf.layers.batch_normalization\n",
        "layer_norm = tf.contrib.layers.layer_norm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJrRN4ujgQh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils/train.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def cross_entropy(logits, labels):\n",
        "    return tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels)\n",
        "\n",
        "def weight_decay(decay, var_list=None):\n",
        "    var_list = tf.trainable_variables() if var_list is None else var_list\n",
        "    return decay*tf.add_n([tf.nn.l2_loss(var) for var in var_list])\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "def get_train_op(optim, loss, global_step=None, clip=None, var_list=None):\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        grad_and_vars = optim.compute_gradients(loss, var_list=var_list)\n",
        "        if clip is not None:\n",
        "            grad_and_vars = [((None if grad is None \\\n",
        "                    else tf.clip_by_norm(grad, clip)), var) \\\n",
        "                    for grad, var in grad_and_vars]\n",
        "        train_op = optim.apply_gradients(grad_and_vars, global_step=global_step)\n",
        "        return train_op\n",
        "\n",
        "# copied from https://stackoverflow.com/a/38580201\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    mem_thres = 0.3*max([x.memory_limit for x in local_device_protos \\\n",
        "            if x.device_type=='GPU'])\n",
        "    return [x.name for x in local_device_protos if x.device_type=='GPU' \\\n",
        "            and x.memory_limit > mem_thres]\n",
        "\n",
        "def average_gradients(tower_grads):\n",
        "  \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
        "\n",
        "  Note that this function provides a synchronization point across all towers.\n",
        "\n",
        "  Args:\n",
        "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
        "      is over individual gradients. The inner list is over the gradient\n",
        "      calculation for each tower.\n",
        "  Returns:\n",
        "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
        "     across all towers.\n",
        "  \"\"\"\n",
        "  average_grads = []\n",
        "  for grad_and_vars in zip(*tower_grads):\n",
        "    # Note that each grad_and_vars looks like the following:\n",
        "    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
        "    grads = []\n",
        "    for g, _ in grad_and_vars:\n",
        "      # Add 0 dimension to the gradients to represent the tower.\n",
        "      expanded_g = tf.expand_dims(g, 0)\n",
        "\n",
        "      # Append on a 'tower' dimension which we will average over below.\n",
        "      grads.append(expanded_g)\n",
        "\n",
        "    # Average over the 'tower' dimension.\n",
        "    grad = tf.concat(axis=0, values=grads)\n",
        "    grad = tf.reduce_mean(grad, 0)\n",
        "\n",
        "    # Keep in mind that the Variables are redundant because they are shared\n",
        "    # across towers. So .. we will just return the first tower's pointer to\n",
        "    # the Variable.\n",
        "    v = grad_and_vars[0][1]\n",
        "    grad_and_var = (grad, v)\n",
        "    average_grads.append(grad_and_var)\n",
        "  return average_grads\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg7YnEexflxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from layers import *\n",
        "#from utils.train import *\n",
        "\n",
        "def lenet_dense(x, y, training, name='lenet', reuse=None,\n",
        "        dropout=None, **dropout_kwargs):\n",
        "    dropout_ = lambda x, subname: x if dropout is None else \\\n",
        "            dropout(x, training, name=name+subname, reuse=reuse,\n",
        "                    **dropout_kwargs)\n",
        "    x = dense(dropout_(x, '/dropout1'), 500, activation=relu,\n",
        "            name=name+'/dense1', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout2'), 300, activation=relu,\n",
        "            name=name+'/dense2', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout3'), 10, name=name+'/dense3', reuse=reuse)\n",
        "\n",
        "    net = {}\n",
        "    all_vars = tf.get_collection('variables', scope=name)\n",
        "    net['qpi_vars'] = [v for v in all_vars if 'qpi_vars' in v.name]\n",
        "    net['pzx_vars'] = [v for v in all_vars if 'pzx_vars' in v.name]\n",
        "    net['weights'] = [v for v in all_vars \\\n",
        "            if 'qpi_vars' not in v.name and 'pzx_vars' not in v.name]\n",
        "\n",
        "    net['cent'] = cross_entropy(x, y)\n",
        "    net['wd'] = weight_decay(1e-4, var_list=net['weights'])\n",
        "    net['acc'] = accuracy(x, y)\n",
        "\n",
        "    prefix = 'train_' if training else 'test_'\n",
        "    net['kl'] = tf.get_collection('kl')\n",
        "    net['pi'] = tf.get_collection(prefix+'pi')\n",
        "    net['n_active'] = tf.get_collection(prefix+'n_active')\n",
        "\n",
        "    return net\n",
        "\n",
        "def lenet_conv(x, y, training, name='lenet', reuse=None,\n",
        "        dropout=None, **dropout_kwargs):\n",
        "    dropout_ = lambda x, subname: x if dropout is None else \\\n",
        "            dropout(x, training, name=name+subname, reuse=reuse,\n",
        "                    **dropout_kwargs)\n",
        "    x = tf.reshape(x, [-1, 1, 28, 28])\n",
        "    x = conv(x, 20, 5, name=name+'/conv1', reuse=reuse)\n",
        "    x = relu(dropout_(x, '/dropout1'))\n",
        "    x = pool(x, name=name+'/pool1')\n",
        "    x = conv(x, 50, 5, name=name+'/conv2', reuse=reuse)\n",
        "    x = relu(dropout_(x, '/dropout2'))\n",
        "    x = pool(x, name=name+'/pool2')\n",
        "    x = flatten(x)\n",
        "    x = dense(dropout_(x, '/dropout3'), 500, activation=relu,\n",
        "            name=name+'/dense1', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout4'), 10, name=name+'/dense2', reuse=reuse)\n",
        "\n",
        "    net = {}\n",
        "    all_vars = tf.get_collection('variables', scope=name)\n",
        "    net['qpi_vars'] = [v for v in all_vars if 'qpi_vars' in v.name]\n",
        "    net['pzx_vars'] = [v for v in all_vars if 'pzx_vars' in v.name]\n",
        "    net['weights'] = [v for v in all_vars \\\n",
        "            if 'qpi_vars' not in v.name and 'pzx_vars' not in v.name]\n",
        "\n",
        "    net['cent'] = cross_entropy(x, y)\n",
        "    net['wd'] = weight_decay(1e-4, var_list=net['weights'])\n",
        "    net['acc'] = accuracy(x, y)\n",
        "\n",
        "    prefix = 'train_' if training else 'test_'\n",
        "    net['kl'] = tf.get_collection('kl')\n",
        "    net['pi'] = tf.get_collection(prefix+'pi')\n",
        "    net['n_active'] = tf.get_collection(prefix+'n_active')\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-CieEeVgpnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# @MISC {1446110,\n",
        "# TITLE = {Approximating the Digamma function},\n",
        "# AUTHOR = {njuffa (https://math.stackexchange.com/users/114200/njuffa)},\n",
        "# HOWPUBLISHED = {Mathematics Stack Exchange},\n",
        "# NOTE = {URL:https://math.stackexchange.com/q/1446110 (version: 2015-09-22)},\n",
        "# EPRINT = {https://math.stackexchange.com/q/1446110},\n",
        "# URL = {https://math.stackexchange.com/q/1446110}}\n",
        "\n",
        "def digamma_approx(x):\n",
        "    def digamma_over_one(x):\n",
        "        return tf.log(x + 0.4849142940227510) \\\n",
        "                - 1/(1.0271785180163817*x)\n",
        "    return digamma_over_one(x+1) - 1./x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJjoi_-lga15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model/bbdropout.py\n",
        "#from layers import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.distributions import RelaxedBernoulli\n",
        "import numpy as np\n",
        "\n",
        "#digamma = tf.digamma\n",
        "#from digamma import digamma_approx as digamma_approx\n",
        "lgamma = tf.lgamma\n",
        "Euler = 0.577215664901532\n",
        "\n",
        "def bbdropout(x, training,\n",
        "        alpha=1e-4, thres=1e-2, a_init=-1., tau=1e-1, center_init=1.0,\n",
        "        approx_digamma=True, scale_kl=None, dep=False,\n",
        "        unit_scale=True, collect=True,\n",
        "        name='bbdropout', reuse=None):\n",
        "\n",
        "    N = tf.shape(x)[0]\n",
        "    K = x.shape[1].value\n",
        "    is_conv = len(x.shape)==4\n",
        "    log = lambda x: tf.log(x + 1e-20)\n",
        "\n",
        "\n",
        "    with tf.variable_scope(name+'/qpi_vars', reuse=reuse):\n",
        "        with tf.device('/cpu:0'):\n",
        "            a = softplus(tf.get_variable('a_uc', shape=[K],\n",
        "                initializer=tf.constant_initializer(a_init)))\n",
        "            b = softplus(tf.get_variable('b_uc', shape=[K]))\n",
        "\n",
        "    _digamma = digamma_approx \n",
        "    kl = (a-alpha)/a * (-Euler - _digamma(b) - 1/b) \\\n",
        "            + log(a*b) - log(alpha) - (b-1)/b\n",
        "    pi = (1 - tf.random_uniform([K])**(1/b))**(1/a) if training else \\\n",
        "            b*tf.exp(lgamma(1+1/a) + lgamma(b) - lgamma(1+1/a+b))\n",
        "\n",
        "    def hard_sigmoid(x):\n",
        "        return tf.clip_by_value(x, thres, 1-thres)\n",
        "\n",
        "    if dep:\n",
        "        with tf.variable_scope(name+'/pzx_vars', reuse=reuse):\n",
        "            hid = global_avg_pool(x) if is_conv else x\n",
        "            hid = tf.stop_gradient(hid)\n",
        "            with tf.device('/cpu:0'):\n",
        "                hid = layer_norm(hid, scale=False, center=False)\n",
        "                scale = tf.get_variable('scale', shape=[1 if unit_scale else K],\n",
        "                        initializer=tf.ones_initializer())\n",
        "                center = tf.get_variable('center', shape=[K],\n",
        "                        initializer=tf.constant_initializer(center_init))\n",
        "            hid = scale*hid + center\n",
        "        if training:\n",
        "            pi = pi * hard_sigmoid(hid + tf.random_normal(shape=tf.shape(hid)))\n",
        "            z = RelaxedBernoulli(tau, logits=logit(pi)).sample()\n",
        "        else:\n",
        "            pi = pi * hard_sigmoid(hid)\n",
        "            z = tf.where(tf.greater(pi, thres), pi, tf.zeros_like(pi))\n",
        "        #n_active = tf.reduce_mean(\n",
        "        #        tf.reduce_sum(tf.cast(tf.greater(pi, thres), tf.int32), 1))\n",
        "        n_active = tf.reduce_sum(tf.cast(tf.greater(pi, thres), tf.int32), 1)\n",
        "        n_active = tf.reduce_sum(n_active)/N\n",
        "    else:\n",
        "        if training:\n",
        "            z = RelaxedBernoulli(tau, logits=logit(pi)).sample(N)\n",
        "        else:\n",
        "            pi_ = tf.where(tf.greater(pi, thres), pi, tf.zeros_like(pi))\n",
        "            z = tf.tile(tf.expand_dims(pi_, 0), [N, 1])\n",
        "        n_active = tf.reduce_sum(tf.cast(tf.greater(pi, thres), tf.int32))\n",
        "\n",
        "    if scale_kl is None:\n",
        "        kl = tf.reduce_sum(kl)\n",
        "    else:\n",
        "        kl = scale_kl * tf.reduce_mean(kl)\n",
        "\n",
        "    if collect:\n",
        "        if reuse is not True:\n",
        "            tf.add_to_collection('kl', kl)\n",
        "        prefix = 'train_' if training else 'test_'\n",
        "        tf.add_to_collection(prefix+'pi', pi)\n",
        "        tf.add_to_collection(prefix+'n_active', n_active)\n",
        "\n",
        "    z = tf.reshape(z, ([-1, K, 1, 1] if is_conv else [-1, K]))\n",
        "    return x*z\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6AMDZDHhK3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils/accmulator.py\n",
        "from __future__ import print_function\n",
        "\n",
        "class Accumulator():\n",
        "    def __init__(self, *args):\n",
        "        self.args = args\n",
        "        self.argdict = {}\n",
        "        for i, arg in enumerate(args):\n",
        "            self.argdict[arg] = i\n",
        "        self.sums = [0]*len(args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def accum(self, val):\n",
        "        val = [val] if type(val) is not list else val\n",
        "        val = [v for v in val if v is not None]\n",
        "        assert(len(val) == len(self.args))\n",
        "        for i in range(len(val)):\n",
        "            self.sums[i] += val[i]\n",
        "        self.cnt += 1\n",
        "\n",
        "    def clear(self):\n",
        "        self.sums = [0]*len(self.args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def get(self, arg, avg=True):\n",
        "        i = self.argdict.get(arg, -1)\n",
        "        assert(i is not -1)\n",
        "        return (self.sums[i]/self.cnt if avg else self.sums[i])\n",
        "\n",
        "    def print_(self, header=None, epoch=None, it=None, time=None,\n",
        "            logfile=None, do_not_print=[], as_int=[],\n",
        "            avg=True):\n",
        "        line = '' if header is None else header + ': '\n",
        "        if epoch is not None:\n",
        "            line += ('epoch %d, ' % epoch)\n",
        "        if it is not None:\n",
        "            line += ('iter %d, ' % it)\n",
        "        if time is not None:\n",
        "            line += ('(%.3f secs), ' % time)\n",
        "\n",
        "        args = [arg for arg in self.args if arg not in do_not_print]\n",
        "\n",
        "        for arg in args[:-1]:\n",
        "            val = self.sums[self.argdict[arg]]\n",
        "            if avg:\n",
        "                val /= self.cnt\n",
        "            if arg in as_int:\n",
        "                line += ('%s %d, ' % (arg, int(val)))\n",
        "            else:\n",
        "                line += ('%s %f, ' % (arg, val))\n",
        "        val = self.sums[self.argdict[args[-1]]]\n",
        "        if avg:\n",
        "            val /= self.cnt\n",
        "        if arg in as_int:\n",
        "            line += ('%s %d, ' % (arg, int(val)))\n",
        "        else:\n",
        "            line += ('%s %f' % (args[-1], val))\n",
        "        print(line)\n",
        "\n",
        "        if logfile is not None:\n",
        "            logfile.write(line + '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q21ZNy8bhTpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "#from paths import MNIST_PATH\n",
        "MNIST_PATH = './mnist'\n",
        "\n",
        "def mnist_input(batch_size):\n",
        "    mnist = input_data.read_data_sets(MNIST_PATH, one_hot=True, validation_size=0)\n",
        "    n_train_batches = mnist.train.num_examples/batch_size\n",
        "    n_test_batches = mnist.test.num_examples/batch_size\n",
        "    return mnist, n_train_batches, n_test_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz6djPqUWps9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b724afca-4f1b-44fb-ae48-f2eced0d44b5"
      },
      "source": [
        "from __future__ import print_function\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "#from model.lenet import lenet_dense\n",
        "#from model.bbdropout import bbdropout\n",
        "#from utils.accumulator import Accumulator\n",
        "#from utils.train import *\n",
        "#from utils.mnist import mnist_input\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import csv\n",
        "from pylab import *\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--batch_size', type=int, default=100)\n",
        "# parser.add_argument('--n_epochs', type=int, default=200)\n",
        "# parser.add_argument('--save_freq', type=int, default=20)\n",
        "# parser.add_argument('--savedir', type=str, default=None)\n",
        "# parser.add_argument('--pretraindir', type=str, default=None)\n",
        "# parser.add_argument('--mode', type=str, default='train')\n",
        "# parser.add_argument('--gpu_num', type=int, default=0)\n",
        "# parser.add_argument('--csvfn', type=str, default=None)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_num)\n",
        "\n",
        "pretraindir = './results/pretrained' \n",
        "savedir = './results/bbdropout/sample_run' \n",
        "if not os.path.isdir(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "batch_size = 100\n",
        "n_epochs = 60\n",
        "save_freq = 20\n",
        "mnist, n_train_batches, n_test_batches = mnist_input(batch_size)\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "N = mnist.train.num_examples\n",
        "dropout = bbdropout\n",
        "net = lenet_dense(x, y, True, dropout=dropout)\n",
        "tnet = lenet_dense(x, y, False, reuse=True, dropout=dropout)\n",
        "\n",
        "\n",
        "def train():\n",
        "    loss = net['cent'] + tf.add_n(net['kl'])/float(N) + net['wd']\n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    bdr = [int(n_train_batches*(n_epochs-1)*r) for r in [0.5, 0.75]]\n",
        "    vals = [1e-2, 1e-3, 1e-4]\n",
        "    lr = tf.train.piecewise_constant(tf.cast(global_step, tf.int32), bdr, vals)\n",
        "    train_op1 = tf.train.AdamOptimizer(lr).minimize(loss,\n",
        "            var_list=net['qpi_vars'], global_step=global_step)\n",
        "    train_op2 = tf.train.AdamOptimizer(0.1*lr).minimize(loss,\n",
        "            var_list=net['weights'])\n",
        "    train_op = tf.group(train_op1, train_op2)\n",
        "\n",
        "    pretrain_saver = tf.train.Saver(net['weights'])\n",
        "    saver = tf.train.Saver(net['weights']+net['qpi_vars'])\n",
        "    logfile = open(os.path.join(savedir, 'train.log'), 'w', 0)\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    pretrain_saver.restore(sess, os.path.join(pretraindir, 'model'))\n",
        "\n",
        "    train_logger = Accumulator('cent', 'acc')\n",
        "    train_to_run = [train_op, net['cent'], net['acc']]\n",
        "    test_logger = Accumulator('cent', 'acc')\n",
        "    test_to_run = [tnet['cent'], tnet['acc']]\n",
        "    for i in range(n_epochs):\n",
        "        line = 'Epoch %d start, learning rate %f' % (i+1, sess.run(lr))\n",
        "        print(line)\n",
        "        logfile.write(line + '\\n')\n",
        "        train_logger.clear()\n",
        "        start = time.time()\n",
        "        for j in range(n_train_batches):\n",
        "            bx, by = mnist.train.next_batch(batch_size)\n",
        "            train_logger.accum(sess.run(train_to_run, {x:bx, y:by}))\n",
        "        train_logger.print_(header='train', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "\n",
        "        test_logger.clear()\n",
        "        for j in range(n_test_batches):\n",
        "            bx, by = mnist.test.next_batch(batch_size)\n",
        "            test_logger.accum(sess.run(test_to_run, {x:bx, y:by}))\n",
        "        test_logger.print_(header='test', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "        line = 'kl: ' + str(sess.run(tnet['kl'])) + '\\n'\n",
        "        line += 'n_active: ' + str(sess.run(tnet['n_active'])) + '\\n'\n",
        "        print(line)\n",
        "        logfile.write(line+'\\n')\n",
        "\n",
        "        if (i+1)% save_freq == 0:\n",
        "            saver.save(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    logfile.close()\n",
        "    saver.save(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "def test():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "    logger = Accumulator('cent', 'acc')\n",
        "    to_run = [tnet['cent'], tnet['acc']]\n",
        "    for j in range(n_test_batches):\n",
        "        bx, by = mnist.test.next_batch(batch_size)\n",
        "        logger.accum(sess.run(to_run, {x:bx, y:by}))\n",
        "    logger.print_(header='test')\n",
        "    line = 'kl: ' + str(sess.run(tnet['kl'])) + '\\n'\n",
        "    line += 'n_active: ' + str(sess.run(tnet['n_active'])) + '\\n'\n",
        "    print(line)\n",
        "\n",
        "def visualize():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    n_drop = len(tnet['n_active'])\n",
        "    fig = figure('pi')\n",
        "    axarr = fig.subplots(n_drop)\n",
        "    for i in range(n_drop):\n",
        "        np_pi = sess.run(tnet['pi'][i]).reshape((1,-1))\n",
        "        im = axarr[i].imshow(np_pi, cmap='Blues', aspect='auto')\n",
        "        axarr[i].yaxis.set_visible(False)\n",
        "        axarr[i].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        if i == n_drop-1:\n",
        "            axarr[i].set_xlabel('neurons')\n",
        "        fig.colorbar(im, ax=axarr[i])\n",
        "    show()\n",
        "\n",
        "def record():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "    logger = Accumulator('cent', 'acc')\n",
        "    to_run = [tnet['cent'], tnet['acc']]\n",
        "    for j in range(n_test_batches):\n",
        "        bx, by = mnist.test.next_batch(batch_size)\n",
        "        logger.accum(sess.run(to_run, {x:bx, y:by}))\n",
        "    np_n_active = sess.run(tnet['n_active'])\n",
        "\n",
        "    if not os.path.isdir('../../records'):\n",
        "        os.makedirs('../../records')\n",
        "    csvfn = os.path.join('../../records',\n",
        "            'bbdropout_lenet_dense.csv' if args.csvfn is None else args.csvfn)\n",
        "\n",
        "    if csvfn is not None:\n",
        "        flag = 'a' if os.path.exists(csvfn) else 'w'\n",
        "        with open(csvfn, flag) as f:\n",
        "            writer = csv.writer(f)\n",
        "            if flag=='w':\n",
        "                writer.writerow(['savedir', 'cent', 'acc', 'n_active'])\n",
        "            line = [savedir]\n",
        "            line.append('%.4f' % logger.get('cent'))\n",
        "            line.append('%.4f' % logger.get('acc'))\n",
        "            line.append('-'.join(str(x) for x in np_n_active))\n",
        "            writer.writerow(line)\n",
        "\n",
        "# if __name__=='__main__':\n",
        "#     if args.mode == 'train':\n",
        "#         train()\n",
        "#     elif args.mode == 'test':\n",
        "#         test()\n",
        "#     elif args.mode == 'vis':\n",
        "#         visualize()\n",
        "#     elif args.mode == 'record':\n",
        "#         record()\n",
        "#     else:\n",
        "#         raise ValueError('Invalid mode %s' % args.mode)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_0exUrqhel8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a5cd33d-4f93-4388-a668-c0562f669213"
      },
      "source": [
        "train()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 start, learning rate 0.010000\n",
            "train: epoch 1, (6.478 secs), cent 0.409976, acc 0.878650\n",
            "test: epoch 1, (6.870 secs), cent 0.105804, acc 0.967500\n",
            "kl: [4574.4, 3865.5632, 2486.467]\n",
            "n_active: [519, 471, 297]\n",
            "\n",
            "Epoch 2 start, learning rate 0.010000\n",
            "train: epoch 2, (6.014 secs), cent 0.168484, acc 0.947650\n",
            "test: epoch 2, (6.326 secs), cent 0.074276, acc 0.976200\n",
            "kl: [4437.486, 3963.0293, 2558.1135]\n",
            "n_active: [514, 464, 296]\n",
            "\n",
            "Epoch 3 start, learning rate 0.010000\n",
            "train: epoch 3, (5.951 secs), cent 0.126364, acc 0.960767\n",
            "test: epoch 3, (6.265 secs), cent 0.067453, acc 0.979600\n",
            "kl: [4525.461, 4069.4944, 2610.2676]\n",
            "n_active: [504, 461, 296]\n",
            "\n",
            "Epoch 4 start, learning rate 0.010000\n",
            "train: epoch 4, (6.000 secs), cent 0.102697, acc 0.966850\n",
            "test: epoch 4, (6.322 secs), cent 0.066036, acc 0.979100\n",
            "kl: [4588.2847, 4121.4473, 2628.091]\n",
            "n_active: [501, 460, 296]\n",
            "\n",
            "Epoch 5 start, learning rate 0.010000\n",
            "train: epoch 5, (5.955 secs), cent 0.087019, acc 0.971967\n",
            "test: epoch 5, (6.300 secs), cent 0.063670, acc 0.980600\n",
            "kl: [4627.643, 4151.6455, 2633.1265]\n",
            "n_active: [497, 457, 296]\n",
            "\n",
            "Epoch 6 start, learning rate 0.010000\n",
            "train: epoch 6, (6.029 secs), cent 0.078252, acc 0.975067\n",
            "test: epoch 6, (6.346 secs), cent 0.063747, acc 0.979500\n",
            "kl: [4650.1973, 4146.7925, 2650.4407]\n",
            "n_active: [492, 455, 296]\n",
            "\n",
            "Epoch 7 start, learning rate 0.010000\n",
            "train: epoch 7, (6.046 secs), cent 0.076384, acc 0.975500\n",
            "test: epoch 7, (6.356 secs), cent 0.061033, acc 0.981500\n",
            "kl: [4654.3228, 4156.775, 2645.6494]\n",
            "n_active: [485, 450, 296]\n",
            "\n",
            "Epoch 8 start, learning rate 0.010000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-5387778862a5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mtrain_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_to_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         train_logger.print_(header='train', epoch=i+1,\n\u001b[1;32m     79\u001b[0m                 time=time.time()-start, logfile=logfile)\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYDT-eQ1kwQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0xBULDNoNG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLRsNN4loQvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}