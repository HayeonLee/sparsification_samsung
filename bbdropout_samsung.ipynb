{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bbdropout_samsung",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayeonLee/sparsification_samsung/blob/master/bbdropout_samsung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD6bQj0iVZM9",
        "colab_type": "text"
      },
      "source": [
        "# Network Sparsification Example : Beta-Bernoulli Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSc0p6Y6VvoL",
        "colab_type": "text"
      },
      "source": [
        "[Adaptive Network Sparsification with Dependent Variational Beta-Bernoulli Dropout](https://arxiv.org/abs/1805.10896)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP0DgMlDV64u",
        "colab_type": "text"
      },
      "source": [
        "## Import Tensorflow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNpLz5neUrqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "15fc2889-467e-4867-a8a1-3aac8f6af890"
      },
      "source": [
        "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#기존에 설치된 다른 버전의 tensorflow를 제거합니다.\n",
        "!pip uninstall tensorboard -y\n",
        "!pip uninstall tensorflow-gpu -y\n",
        "!pip uninstall tensorflow -y\n",
        "#tensorflow gpu 버전을 설치합니다\n",
        "!pip install tensorflow-gpu==1.14"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorboard-1.14.0:\n",
            "  Successfully uninstalled tensorboard-1.14.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "Uninstalling tensorflow-1.14.0:\n",
            "  Successfully uninstalled tensorflow-1.14.0\n",
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/6d/2348df00a34baaabdef0fdb4f46f962f7a8a6720362c26c3a44a249767ea/tensorflow_gpu-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 60kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow-gpu==1.14)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/37/e6a7af1c92c5b68fb427f853b06164b56ea92126bcfd87784334ec5e4d42/tensorboard-1.14.0-py2-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.33.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.1.7)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (1.0.post1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==1.14) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.14) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.14) (5.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n",
            "Installing collected packages: tensorboard, tensorflow-gpu\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68oyuhXNWEQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a5918f0b-3020-405d-b09e-f70855834a48"
      },
      "source": [
        "import tensorflow as tf # tensorflow를 import해줍니다\n",
        "tf.__version__ # 내가 사용할 tensorflow의 버전을 나타냅니다"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkYfIMpyYdPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "23520527-417f-4661-dd60-e36e236f8d10"
      },
      "source": [
        "# pretrain된 lenet의 체크포인트 파일을 가져옵니다.\n",
        "!mkdir -p results/\n",
        "!wget -O lenet_dense_pretrained.zip https://github.com/HayeonLee/sparsification_samsung/blob/master/lenet_dense_pretrained.zip?raw=true\n",
        "!unzip lenet_dense_pretrained.zip -d results/\n",
        "!rm lenet_dense_pretrained.zip\n",
        "!ls\n",
        "!ls results/pretrained/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-26 16:19:17--  https://github.com/HayeonLee/sparsification_samsung/blob/master/lenet_dense_pretrained.zip?raw=true\n",
            "Resolving github.com (github.com)... 52.192.72.89\n",
            "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/HayeonLee/sparsification_samsung/raw/master/lenet_dense_pretrained.zip [following]\n",
            "--2019-09-26 16:19:18--  https://github.com/HayeonLee/sparsification_samsung/raw/master/lenet_dense_pretrained.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/HayeonLee/sparsification_samsung/master/lenet_dense_pretrained.zip [following]\n",
            "--2019-09-26 16:19:18--  https://raw.githubusercontent.com/HayeonLee/sparsification_samsung/master/lenet_dense_pretrained.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2080208 (2.0M) [application/zip]\n",
            "Saving to: ‘lenet_dense_pretrained.zip’\n",
            "\n",
            "lenet_dense_pretrai 100%[===================>]   1.98M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2019-09-26 16:19:19 (227 MB/s) - ‘lenet_dense_pretrained.zip’ saved [2080208/2080208]\n",
            "\n",
            "Archive:  lenet_dense_pretrained.zip\n",
            "  inflating: results/pretrained/checkpoint  \n",
            "  inflating: results/pretrained/model.data-00000-of-00001  \n",
            "  inflating: results/pretrained/model.index  \n",
            "  inflating: results/pretrained/model.meta  \n",
            "results  sample_data\n",
            "checkpoint  model.data-00000-of-00001  model.index  model.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR4m7OXGAt8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 라이브러리를 임포트합니다.\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import os\n",
        "from pylab import *\n",
        "import numpy as np\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.contrib.distributions import RelaxedBernoulli\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ0R62L8WvSu",
        "colab_type": "text"
      },
      "source": [
        "## Define the functions and utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFCAzQYqfrzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 자주 쓰는 텐서플로우 함수의 약어를 지정합니다.\n",
        "logit = lambda x: tf.log(x + 1e-20) - tf.log(1-x + 1e-20)\n",
        "softplus = tf.nn.softplus\n",
        "relu = tf.nn.relu\n",
        "\n",
        "dense = tf.layers.dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJrRN4ujgQh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils/train.py\n",
        "# 필요한 함수를 정의합니다.\n",
        "def cross_entropy(logits, labels):\n",
        "    return tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels)\n",
        "\n",
        "def weight_decay(decay, var_list=None):\n",
        "    var_list = tf.trainable_variables() if var_list is None else var_list\n",
        "    return decay*tf.add_n([tf.nn.l2_loss(var) for var in var_list])\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "  \n",
        "def digamma_approx(x):\n",
        "# @MISC {1446110,\n",
        "# TITLE = {Approximating the Digamma function},\n",
        "# AUTHOR = {njuffa (https://math.stackexchange.com/users/114200/njuffa)},\n",
        "# HOWPUBLISHED = {Mathematics Stack Exchange},\n",
        "# NOTE = {URL:https://math.stackexchange.com/q/1446110 (version: 2015-09-22)},\n",
        "# EPRINT = {https://math.stackexchange.com/q/1446110},\n",
        "# URL = {https://math.stackexchange.com/q/1446110}}\n",
        "    def digamma_over_one(x):\n",
        "        return tf.log(x + 0.4849142940227510) \\\n",
        "                - 1/(1.0271785180163817*x)\n",
        "    return digamma_over_one(x+1) - 1./x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9RShCp_DDOz",
        "colab": {}
      },
      "source": [
        "# log를 출력하기 위한 함수를 선언합니다.\n",
        "class Accumulator():\n",
        "    def __init__(self, *args):\n",
        "        self.args = args\n",
        "        self.argdict = {}\n",
        "        for i, arg in enumerate(args):\n",
        "            self.argdict[arg] = i\n",
        "        self.sums = [0]*len(args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def accum(self, val):\n",
        "        val = [val] if type(val) is not list else val\n",
        "        val = [v for v in val if v is not None]\n",
        "        assert(len(val) == len(self.args))\n",
        "        for i in range(len(val)):\n",
        "            self.sums[i] += val[i]\n",
        "        self.cnt += 1\n",
        "\n",
        "    def clear(self):\n",
        "        self.sums = [0]*len(self.args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def get(self, arg, avg=True):\n",
        "        i = self.argdict.get(arg, -1)\n",
        "        assert(i is not -1)\n",
        "        return (self.sums[i]/self.cnt if avg else self.sums[i])\n",
        "\n",
        "    def print_(self, header=None, epoch=None, it=None, time=None,\n",
        "            logfile=None, do_not_print=[], as_int=[],\n",
        "            avg=True):\n",
        "        line = '' if header is None else header + ': '\n",
        "        if epoch is not None:\n",
        "            line += ('epoch %d, ' % epoch)\n",
        "        if it is not None:\n",
        "            line += ('iter %d, ' % it)\n",
        "        if time is not None:\n",
        "            line += ('(%.3f secs), ' % time)\n",
        "\n",
        "        args = [arg for arg in self.args if arg not in do_not_print]\n",
        "\n",
        "        for arg in args[:-1]:\n",
        "            val = self.sums[self.argdict[arg]]\n",
        "            if avg:\n",
        "                val /= self.cnt\n",
        "            if arg in as_int:\n",
        "                line += ('%s %d, ' % (arg, int(val)))\n",
        "            else:\n",
        "                line += ('%s %f, ' % (arg, val))\n",
        "        val = self.sums[self.argdict[args[-1]]]\n",
        "        if avg:\n",
        "            val /= self.cnt\n",
        "        if arg in as_int:\n",
        "            line += ('%s %d, ' % (arg, int(val)))\n",
        "        else:\n",
        "            line += ('%s %f' % (args[-1], val))\n",
        "        print(line)\n",
        "\n",
        "        if logfile is not None:\n",
        "            logfile.write(line + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF1WUmZRIdss",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the dataset: MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q21ZNy8bhTpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MNIST_PATH = './mnist'\n",
        "\n",
        "def mnist_input(batch_size):\n",
        "    mnist = input_data.read_data_sets(MNIST_PATH, one_hot=True, validation_size=0)\n",
        "    n_train_batches = mnist.train.num_examples/batch_size\n",
        "    n_test_batches = mnist.test.num_examples/batch_size\n",
        "    return mnist, n_train_batches, n_test_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3lKC6JjHRKM",
        "colab_type": "text"
      },
      "source": [
        "##Create models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg7YnEexflxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fully connected layers로 구성된 lenet을 선언합니다. \n",
        "def lenet_dense(x, y, training, name='lenet', reuse=None,\n",
        "        dropout=None, **dropout_kwargs):\n",
        "    dropout_ = lambda x, subname: x if dropout is None else \\\n",
        "            dropout(x, training, name=name+subname, reuse=reuse,\n",
        "                    **dropout_kwargs) # bbdropout을 적용합니다.\n",
        "    x = dense(dropout_(x, '/dropout1'), 500, activation=relu,\n",
        "            name=name+'/dense1', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout2'), 300, activation=relu,\n",
        "            name=name+'/dense2', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout3'), 10, name=name+'/dense3', reuse=reuse)\n",
        "\n",
        "    net = {}\n",
        "    all_vars = tf.get_collection('variables', scope=name)\n",
        "    net['qpi_vars'] = [v for v in all_vars if 'qpi_vars' in v.name]\n",
        "    net['pzx_vars'] = [v for v in all_vars if 'pzx_vars' in v.name]\n",
        "    net['weights'] = [v for v in all_vars \\\n",
        "            if 'qpi_vars' not in v.name and 'pzx_vars' not in v.name]\n",
        "\n",
        "    net['cent'] = cross_entropy(x, y)\n",
        "    net['wd'] = weight_decay(1e-4, var_list=net['weights'])\n",
        "    net['acc'] = accuracy(x, y)\n",
        "\n",
        "    prefix = 'train_' if training else 'test_'\n",
        "    net['kl'] = tf.get_collection('kl')\n",
        "    net['pi'] = tf.get_collection(prefix+'pi')\n",
        "    net['n_active'] = tf.get_collection(prefix+'n_active')\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA7g-bi7IP6w",
        "colab_type": "text"
      },
      "source": [
        "## Define the Beta-Bernoulli Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJjoi_-lga15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgamma = tf.lgamma\n",
        "Euler = 0.577215664901532\n",
        "\n",
        "def bbdropout(x, training,\n",
        "        alpha=1e-4, thres=1e-2, a_init=-1., tau=1e-1, center_init=1.0,\n",
        "        approx_digamma=True, scale_kl=None, dep=False,\n",
        "        unit_scale=True, collect=True,\n",
        "        name='bbdropout', reuse=None):\n",
        "\n",
        "    N = tf.shape(x)[0]\n",
        "    K = x.shape[1].value\n",
        "    is_conv = len(x.shape)==4\n",
        "    log = lambda x: tf.log(x + 1e-20)\n",
        "    \n",
        "    # Bata 분포를 근사한 Kumaraswamy 분포에 필요한 파라메터 a,b\n",
        "    with tf.variable_scope(name+'/qpi_vars', reuse=reuse):\n",
        "        with tf.device('/cpu:0'):\n",
        "            a = softplus(tf.get_variable('a_uc', shape=[K],\n",
        "                initializer=tf.constant_initializer(a_init)))\n",
        "            b = softplus(tf.get_variable('b_uc', shape=[K]))\n",
        "    \n",
        "    # 사전 확률 p(Z, pi)와 variational dist. q(Z, pi)의 KL divergence 계산합니다.\n",
        "    _digamma = digamma_approx \n",
        "    kl = (a-alpha)/a * (-Euler - _digamma(b) - 1/b) \\\n",
        "            + log(a*b) - log(alpha) - (b-1)/b\n",
        "    # Kumaraswamy 분포로 부터 pi를 샘플링합니다.\n",
        "    pi = (1 - tf.random_uniform([K])**(1/b))**(1/a) if training else \\\n",
        "            b*tf.exp(lgamma(1+1/a) + lgamma(b) - lgamma(1+1/a+b))\n",
        "    \n",
        "    if training:\n",
        "        # Bernoulli분포를 근사한 continuous relaxation에서 z를 샘플링합니다.\n",
        "        z = RelaxedBernoulli(tau, logits=logit(pi)).sample(N)\n",
        "    else:\n",
        "        pi_ = tf.where(tf.greater(pi, thres), pi, tf.zeros_like(pi))\n",
        "        z = tf.tile(tf.expand_dims(pi_, 0), [N, 1])\n",
        "    # 활성화된 뉴런의 갯수를 계산합니다.\n",
        "    n_active = tf.reduce_sum(tf.cast(tf.greater(pi, thres), tf.int32))\n",
        "\n",
        "    if scale_kl is None:\n",
        "        kl = tf.reduce_sum(kl)\n",
        "    else:\n",
        "        kl = scale_kl * tf.reduce_mean(kl)\n",
        "\n",
        "    if collect:\n",
        "        if reuse is not True:\n",
        "            tf.add_to_collection('kl', kl)\n",
        "        prefix = 'train_' if training else 'test_'\n",
        "        tf.add_to_collection(prefix+'pi', pi)\n",
        "        tf.add_to_collection(prefix+'n_active', n_active)\n",
        "\n",
        "    z = tf.reshape(z, ([-1, K, 1, 1] if is_conv else [-1, K]))\n",
        "    # x에 binary mask z를 적용합니다.\n",
        "    return x*z\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InEG2TA3Iilj",
        "colab_type": "text"
      },
      "source": [
        "## Let's run the code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz6djPqUWps9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3a34503-3aed-4381-d3b1-c950efbbdab5"
      },
      "source": [
        "tf.reset_default_graph() # 기존의 그려진 텐서플로우 그래프를 제거합니다.\n",
        "\n",
        "# 경로 설정\n",
        "pretraindir = './results/pretrained' \n",
        "savedir = './results/bbdropout/sample_run' \n",
        "if not os.path.isdir(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "batch_size = 100\n",
        "n_epochs = 60\n",
        "save_freq = 20\n",
        "# 데이터 로더 mnist\n",
        "mnist, n_train_batches, n_test_batches = mnist_input(batch_size)\n",
        "# x: MNIST 이미지, y: 0~10 one-hot label (shape 선언)\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "N = mnist.train.num_examples\n",
        "# 네트워크 선언\n",
        "dropout = bbdropout\n",
        "net = lenet_dense(x, y, True, dropout=dropout)\n",
        "tnet = lenet_dense(x, y, False, reuse=True, dropout=dropout)\n",
        "\n",
        "def train():\n",
        "    # net['cent']: cross entropy with softmax \n",
        "    # net['kl']: 사전 확률 p(Z, pi)와 variational dist. q(Z, pi)의 KL\n",
        "    loss = net['cent'] + tf.add_n(net['kl'])/float(N) + net['wd'] \n",
        "    \n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    bdr = [int(n_train_batches*(n_epochs-1)*r) for r in [0.5, 0.75]]\n",
        "    vals = [1e-2, 1e-3, 1e-4]\n",
        "    lr = tf.train.piecewise_constant(tf.cast(global_step, tf.int32), bdr, vals)\n",
        "    # Optimizer 선언\n",
        "    train_op1 = tf.train.AdamOptimizer(lr).minimize(loss,\n",
        "            var_list=net['qpi_vars'], global_step=global_step)\n",
        "    train_op2 = tf.train.AdamOptimizer(0.1*lr).minimize(loss,\n",
        "            var_list=net['weights'])\n",
        "    train_op = tf.group(train_op1, train_op2)\n",
        "    \n",
        "    # 모델 저장\n",
        "    pretrain_saver = tf.train.Saver(net['weights'])\n",
        "    saver = tf.train.Saver(net['weights']+net['qpi_vars'])\n",
        "    logfile = open(os.path.join(savedir, 'train.log'), 'w', 0)\n",
        "    \n",
        "    # 세션 선언\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    pretrain_saver.restore(sess, os.path.join(pretraindir, 'model'))\n",
        "    \n",
        "    # loss와 정확도 로그 수집 \n",
        "    train_logger = Accumulator('cent', 'acc')\n",
        "    test_logger = Accumulator('cent', 'acc')\n",
        "    \n",
        "    train_to_run = [train_op, net['cent'], net['acc']]\n",
        "    test_to_run = [tnet['cent'], tnet['acc']]\n",
        "    \n",
        "    for i in range(n_epochs):\n",
        "        line = 'Epoch %d start, learning rate %f' % (i+1, sess.run(lr))\n",
        "        print(line)\n",
        "        logfile.write(line + '\\n')\n",
        "        train_logger.clear()\n",
        "        start = time.time()\n",
        "        for j in range(n_train_batches):\n",
        "            bx, by = mnist.train.next_batch(batch_size)\n",
        "            # bx와 by를 인풋으로 train_to_run 실행하여 net['cent'], net['acc']의 결과를 log로 수집\n",
        "            train_logger.accum(sess.run(train_to_run, {x:bx, y:by}))\n",
        "        train_logger.print_(header='train', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "        \n",
        "        # 테스트\n",
        "        test_logger.clear()\n",
        "        for j in range(n_test_batches):\n",
        "            bx, by = mnist.test.next_batch(batch_size)\n",
        "            test_logger.accum(sess.run(test_to_run, {x:bx, y:by}))\n",
        "        test_logger.print_(header='test', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "        #line = 'kl: ' + str(sess.run(tnet['kl'])) + '\\n'\n",
        "        line += '\\nn_active: ' + str(sess.run(tnet['n_active'])) + '\\n'\n",
        "        print(line)\n",
        "        logfile.write(line+'\\n')\n",
        "\n",
        "        if (i+1)% save_freq == 0:\n",
        "            saver.save(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    logfile.close()\n",
        "    saver.save(sess, os.path.join(savedir, 'model'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0926 16:31:58.631004 140389069313920 deprecation.py:323] From <ipython-input-8-ac30c5e1ee8c>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0926 16:31:58.632335 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0926 16:31:58.633740 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0926 16:31:59.122813 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting ./mnist/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0926 16:31:59.454691 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0926 16:31:59.458105 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting ./mnist/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0926 16:32:00.028064 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0926 16:32:00.263122 140389069313920 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0926 16:32:00.301887 140389069313920 deprecation.py:323] From <ipython-input-11-78bf366fbd39>:30: __init__ (from tensorflow.contrib.distributions.python.ops.relaxed_bernoulli) is deprecated and will be removed after 2018-10-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
            "W0926 16:32:00.307075 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/relaxed_bernoulli.py:190: __init__ (from tensorflow.contrib.distributions.python.ops.logistic) is deprecated and will be removed after 2018-10-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
            "W0926 16:32:00.310295 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/logistic.py:146: __init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "W0926 16:32:00.311320 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/relaxed_bernoulli.py:191: __init__ (from tensorflow.contrib.distributions.python.ops.bijectors.sigmoid) is deprecated and will be removed after 2018-10-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
            "W0926 16:32:00.312275 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/relaxed_bernoulli.py:193: __init__ (from tensorflow.python.ops.distributions.transformed_distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "W0926 16:32:00.487004 140389069313920 deprecation.py:323] From <ipython-input-10-9b94073a2865>:6: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0926 16:32:00.976950 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_0exUrqhel8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee8ae8ee-07ca-4d05-f599-cb69e9412d54"
      },
      "source": [
        "train()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0926 16:32:16.269603 140389069313920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 start, learning rate 0.010000\n",
            "train: epoch 1, (5.398 secs), cent 0.402754, acc 0.879067\n",
            "test: epoch 1, (5.725 secs), cent 0.099237, acc 0.970500\n",
            "Epoch 1 start, learning rate 0.010000\n",
            "n_active: [527, 465, 297]\n",
            "\n",
            "Epoch 2 start, learning rate 0.010000\n",
            "train: epoch 2, (4.405 secs), cent 0.162568, acc 0.949433\n",
            "test: epoch 2, (4.640 secs), cent 0.085808, acc 0.974000\n",
            "Epoch 2 start, learning rate 0.010000\n",
            "n_active: [511, 461, 296]\n",
            "\n",
            "Epoch 3 start, learning rate 0.010000\n",
            "train: epoch 3, (4.439 secs), cent 0.124134, acc 0.960583\n",
            "test: epoch 3, (4.685 secs), cent 0.065669, acc 0.978900\n",
            "Epoch 3 start, learning rate 0.010000\n",
            "n_active: [503, 458, 296]\n",
            "\n",
            "Epoch 4 start, learning rate 0.010000\n",
            "train: epoch 4, (4.334 secs), cent 0.101309, acc 0.968467\n",
            "test: epoch 4, (4.581 secs), cent 0.065359, acc 0.979700\n",
            "Epoch 4 start, learning rate 0.010000\n",
            "n_active: [494, 457, 296]\n",
            "\n",
            "Epoch 5 start, learning rate 0.010000\n",
            "train: epoch 5, (4.369 secs), cent 0.088036, acc 0.972450\n",
            "test: epoch 5, (4.616 secs), cent 0.056935, acc 0.981900\n",
            "Epoch 5 start, learning rate 0.010000\n",
            "n_active: [490, 456, 296]\n",
            "\n",
            "Epoch 6 start, learning rate 0.010000\n",
            "train: epoch 6, (4.377 secs), cent 0.079869, acc 0.974667\n",
            "test: epoch 6, (4.610 secs), cent 0.058174, acc 0.980400\n",
            "Epoch 6 start, learning rate 0.010000\n",
            "n_active: [484, 454, 296]\n",
            "\n",
            "Epoch 7 start, learning rate 0.010000\n",
            "train: epoch 7, (4.330 secs), cent 0.071397, acc 0.976983\n",
            "test: epoch 7, (4.569 secs), cent 0.058715, acc 0.980700\n",
            "Epoch 7 start, learning rate 0.010000\n",
            "n_active: [482, 452, 296]\n",
            "\n",
            "Epoch 8 start, learning rate 0.010000\n",
            "train: epoch 8, (4.353 secs), cent 0.065879, acc 0.978667\n",
            "test: epoch 8, (4.597 secs), cent 0.053961, acc 0.981600\n",
            "Epoch 8 start, learning rate 0.010000\n",
            "n_active: [480, 448, 296]\n",
            "\n",
            "Epoch 9 start, learning rate 0.010000\n",
            "train: epoch 9, (4.351 secs), cent 0.062300, acc 0.980083\n",
            "test: epoch 9, (4.593 secs), cent 0.055510, acc 0.982500\n",
            "Epoch 9 start, learning rate 0.010000\n",
            "n_active: [473, 437, 296]\n",
            "\n",
            "Epoch 10 start, learning rate 0.010000\n",
            "train: epoch 10, (4.340 secs), cent 0.059985, acc 0.980717\n",
            "test: epoch 10, (4.572 secs), cent 0.051932, acc 0.983800\n",
            "Epoch 10 start, learning rate 0.010000\n",
            "n_active: [461, 428, 296]\n",
            "\n",
            "Epoch 11 start, learning rate 0.010000\n",
            "train: epoch 11, (4.322 secs), cent 0.056317, acc 0.982067\n",
            "test: epoch 11, (4.566 secs), cent 0.052128, acc 0.983300\n",
            "Epoch 11 start, learning rate 0.010000\n",
            "n_active: [456, 416, 296]\n",
            "\n",
            "Epoch 12 start, learning rate 0.010000\n",
            "train: epoch 12, (4.330 secs), cent 0.055254, acc 0.982367\n",
            "test: epoch 12, (4.573 secs), cent 0.053772, acc 0.982300\n",
            "Epoch 12 start, learning rate 0.010000\n",
            "n_active: [447, 401, 295]\n",
            "\n",
            "Epoch 13 start, learning rate 0.010000\n",
            "train: epoch 13, (4.374 secs), cent 0.049556, acc 0.983683\n",
            "test: epoch 13, (4.613 secs), cent 0.048401, acc 0.985400\n",
            "Epoch 13 start, learning rate 0.010000\n",
            "n_active: [445, 382, 292]\n",
            "\n",
            "Epoch 14 start, learning rate 0.010000\n",
            "train: epoch 14, (4.403 secs), cent 0.046910, acc 0.984533\n",
            "test: epoch 14, (4.646 secs), cent 0.057604, acc 0.982300\n",
            "Epoch 14 start, learning rate 0.010000\n",
            "n_active: [439, 367, 288]\n",
            "\n",
            "Epoch 15 start, learning rate 0.010000\n",
            "train: epoch 15, (4.421 secs), cent 0.048929, acc 0.984300\n",
            "test: epoch 15, (4.665 secs), cent 0.056435, acc 0.981300\n",
            "Epoch 15 start, learning rate 0.010000\n",
            "n_active: [432, 353, 282]\n",
            "\n",
            "Epoch 16 start, learning rate 0.010000\n",
            "train: epoch 16, (4.368 secs), cent 0.045550, acc 0.985100\n",
            "test: epoch 16, (4.604 secs), cent 0.051414, acc 0.982500\n",
            "Epoch 16 start, learning rate 0.010000\n",
            "n_active: [426, 339, 271]\n",
            "\n",
            "Epoch 17 start, learning rate 0.010000\n",
            "train: epoch 17, (4.343 secs), cent 0.044795, acc 0.985250\n",
            "test: epoch 17, (4.578 secs), cent 0.047952, acc 0.985000\n",
            "Epoch 17 start, learning rate 0.010000\n",
            "n_active: [424, 322, 265]\n",
            "\n",
            "Epoch 18 start, learning rate 0.010000\n",
            "train: epoch 18, (4.342 secs), cent 0.041947, acc 0.985733\n",
            "test: epoch 18, (4.588 secs), cent 0.056476, acc 0.982900\n",
            "Epoch 18 start, learning rate 0.010000\n",
            "n_active: [422, 306, 255]\n",
            "\n",
            "Epoch 19 start, learning rate 0.010000\n",
            "train: epoch 19, (4.351 secs), cent 0.042453, acc 0.985983\n",
            "test: epoch 19, (4.590 secs), cent 0.051670, acc 0.983200\n",
            "Epoch 19 start, learning rate 0.010000\n",
            "n_active: [411, 295, 250]\n",
            "\n",
            "Epoch 20 start, learning rate 0.010000\n",
            "train: epoch 20, (4.319 secs), cent 0.039532, acc 0.986317\n",
            "test: epoch 20, (4.574 secs), cent 0.053079, acc 0.983100\n",
            "Epoch 20 start, learning rate 0.010000\n",
            "n_active: [408, 282, 240]\n",
            "\n",
            "Epoch 21 start, learning rate 0.010000\n",
            "train: epoch 21, (4.340 secs), cent 0.038396, acc 0.987117\n",
            "test: epoch 21, (4.595 secs), cent 0.052974, acc 0.982100\n",
            "Epoch 21 start, learning rate 0.010000\n",
            "n_active: [403, 273, 231]\n",
            "\n",
            "Epoch 22 start, learning rate 0.010000\n",
            "train: epoch 22, (4.386 secs), cent 0.038442, acc 0.987800\n",
            "test: epoch 22, (4.633 secs), cent 0.052723, acc 0.984000\n",
            "Epoch 22 start, learning rate 0.010000\n",
            "n_active: [401, 268, 226]\n",
            "\n",
            "Epoch 23 start, learning rate 0.010000\n",
            "train: epoch 23, (4.363 secs), cent 0.035338, acc 0.988367\n",
            "test: epoch 23, (4.609 secs), cent 0.048748, acc 0.984100\n",
            "Epoch 23 start, learning rate 0.010000\n",
            "n_active: [397, 264, 215]\n",
            "\n",
            "Epoch 24 start, learning rate 0.010000\n",
            "train: epoch 24, (4.322 secs), cent 0.034965, acc 0.988417\n",
            "test: epoch 24, (4.569 secs), cent 0.053808, acc 0.983800\n",
            "Epoch 24 start, learning rate 0.010000\n",
            "n_active: [393, 253, 207]\n",
            "\n",
            "Epoch 25 start, learning rate 0.010000\n",
            "train: epoch 25, (4.466 secs), cent 0.036063, acc 0.988133\n",
            "test: epoch 25, (4.745 secs), cent 0.049318, acc 0.984600\n",
            "Epoch 25 start, learning rate 0.010000\n",
            "n_active: [389, 245, 195]\n",
            "\n",
            "Epoch 26 start, learning rate 0.010000\n",
            "train: epoch 26, (4.408 secs), cent 0.035572, acc 0.988417\n",
            "test: epoch 26, (4.654 secs), cent 0.056958, acc 0.982400\n",
            "Epoch 26 start, learning rate 0.010000\n",
            "n_active: [386, 231, 186]\n",
            "\n",
            "Epoch 27 start, learning rate 0.010000\n",
            "train: epoch 27, (4.328 secs), cent 0.033945, acc 0.988567\n",
            "test: epoch 27, (4.573 secs), cent 0.053176, acc 0.984000\n",
            "Epoch 27 start, learning rate 0.010000\n",
            "n_active: [385, 227, 181]\n",
            "\n",
            "Epoch 28 start, learning rate 0.010000\n",
            "train: epoch 28, (4.312 secs), cent 0.034091, acc 0.988233\n",
            "test: epoch 28, (4.553 secs), cent 0.049951, acc 0.985000\n",
            "Epoch 28 start, learning rate 0.010000\n",
            "n_active: [378, 219, 177]\n",
            "\n",
            "Epoch 29 start, learning rate 0.010000\n",
            "train: epoch 29, (4.372 secs), cent 0.032789, acc 0.988933\n",
            "test: epoch 29, (4.622 secs), cent 0.052371, acc 0.984000\n",
            "Epoch 29 start, learning rate 0.010000\n",
            "n_active: [376, 217, 173]\n",
            "\n",
            "Epoch 30 start, learning rate 0.010000\n",
            "train: epoch 30, (4.332 secs), cent 0.029429, acc 0.990200\n",
            "test: epoch 30, (4.574 secs), cent 0.045802, acc 0.986500\n",
            "Epoch 30 start, learning rate 0.010000\n",
            "n_active: [374, 213, 168]\n",
            "\n",
            "Epoch 31 start, learning rate 0.001000\n",
            "train: epoch 31, (4.446 secs), cent 0.020523, acc 0.993850\n",
            "test: epoch 31, (4.717 secs), cent 0.043160, acc 0.987100\n",
            "Epoch 31 start, learning rate 0.001000\n",
            "n_active: [374, 212, 167]\n",
            "\n",
            "Epoch 32 start, learning rate 0.001000\n",
            "train: epoch 32, (4.560 secs), cent 0.017883, acc 0.994533\n",
            "test: epoch 32, (4.828 secs), cent 0.043854, acc 0.987300\n",
            "Epoch 32 start, learning rate 0.001000\n",
            "n_active: [374, 212, 167]\n",
            "\n",
            "Epoch 33 start, learning rate 0.001000\n",
            "train: epoch 33, (4.536 secs), cent 0.018156, acc 0.994350\n",
            "test: epoch 33, (4.794 secs), cent 0.042305, acc 0.987200\n",
            "Epoch 33 start, learning rate 0.001000\n",
            "n_active: [374, 212, 167]\n",
            "\n",
            "Epoch 34 start, learning rate 0.001000\n",
            "train: epoch 34, (4.356 secs), cent 0.017919, acc 0.994417\n",
            "test: epoch 34, (4.604 secs), cent 0.042973, acc 0.987300\n",
            "Epoch 34 start, learning rate 0.001000\n",
            "n_active: [374, 212, 167]\n",
            "\n",
            "Epoch 35 start, learning rate 0.001000\n",
            "train: epoch 35, (4.389 secs), cent 0.016401, acc 0.994833\n",
            "test: epoch 35, (4.634 secs), cent 0.042889, acc 0.987100\n",
            "Epoch 35 start, learning rate 0.001000\n",
            "n_active: [374, 212, 165]\n",
            "\n",
            "Epoch 36 start, learning rate 0.001000\n",
            "train: epoch 36, (4.368 secs), cent 0.017066, acc 0.994467\n",
            "test: epoch 36, (4.606 secs), cent 0.043097, acc 0.986800\n",
            "Epoch 36 start, learning rate 0.001000\n",
            "n_active: [373, 211, 165]\n",
            "\n",
            "Epoch 37 start, learning rate 0.001000\n",
            "train: epoch 37, (4.340 secs), cent 0.016671, acc 0.995050\n",
            "test: epoch 37, (4.586 secs), cent 0.043349, acc 0.987200\n",
            "Epoch 37 start, learning rate 0.001000\n",
            "n_active: [371, 209, 165]\n",
            "\n",
            "Epoch 38 start, learning rate 0.001000\n",
            "train: epoch 38, (4.359 secs), cent 0.015671, acc 0.995383\n",
            "test: epoch 38, (4.605 secs), cent 0.043003, acc 0.987600\n",
            "Epoch 38 start, learning rate 0.001000\n",
            "n_active: [370, 207, 165]\n",
            "\n",
            "Epoch 39 start, learning rate 0.001000\n",
            "train: epoch 39, (4.351 secs), cent 0.015451, acc 0.995033\n",
            "test: epoch 39, (4.590 secs), cent 0.043327, acc 0.987100\n",
            "Epoch 39 start, learning rate 0.001000\n",
            "n_active: [369, 204, 161]\n",
            "\n",
            "Epoch 40 start, learning rate 0.001000\n",
            "train: epoch 40, (4.394 secs), cent 0.015864, acc 0.995150\n",
            "test: epoch 40, (4.632 secs), cent 0.043062, acc 0.987100\n",
            "Epoch 40 start, learning rate 0.001000\n",
            "n_active: [367, 201, 160]\n",
            "\n",
            "Epoch 41 start, learning rate 0.001000\n",
            "train: epoch 41, (4.352 secs), cent 0.016299, acc 0.994883\n",
            "test: epoch 41, (4.592 secs), cent 0.043705, acc 0.987300\n",
            "Epoch 41 start, learning rate 0.001000\n",
            "n_active: [367, 200, 159]\n",
            "\n",
            "Epoch 42 start, learning rate 0.001000\n",
            "train: epoch 42, (4.366 secs), cent 0.015751, acc 0.994917\n",
            "test: epoch 42, (4.610 secs), cent 0.042515, acc 0.987600\n",
            "Epoch 42 start, learning rate 0.001000\n",
            "n_active: [367, 200, 158]\n",
            "\n",
            "Epoch 43 start, learning rate 0.001000\n",
            "train: epoch 43, (4.323 secs), cent 0.015592, acc 0.995183\n",
            "test: epoch 43, (4.563 secs), cent 0.043442, acc 0.987300\n",
            "Epoch 43 start, learning rate 0.001000\n",
            "n_active: [366, 200, 158]\n",
            "\n",
            "Epoch 44 start, learning rate 0.001000\n",
            "train: epoch 44, (4.345 secs), cent 0.016037, acc 0.995683\n",
            "test: epoch 44, (4.587 secs), cent 0.043839, acc 0.987300\n",
            "Epoch 44 start, learning rate 0.001000\n",
            "n_active: [365, 200, 156]\n",
            "\n",
            "Epoch 45 start, learning rate 0.001000\n",
            "train: epoch 45, (4.354 secs), cent 0.014934, acc 0.995633\n",
            "test: epoch 45, (4.594 secs), cent 0.042619, acc 0.987500\n",
            "Epoch 45 start, learning rate 0.001000\n",
            "n_active: [365, 200, 155]\n",
            "\n",
            "Epoch 46 start, learning rate 0.000100\n",
            "train: epoch 46, (4.370 secs), cent 0.015047, acc 0.995533\n",
            "test: epoch 46, (4.630 secs), cent 0.042522, acc 0.987800\n",
            "Epoch 46 start, learning rate 0.000100\n",
            "n_active: [365, 200, 155]\n",
            "\n",
            "Epoch 47 start, learning rate 0.000100\n",
            "train: epoch 47, (4.354 secs), cent 0.014365, acc 0.996050\n",
            "test: epoch 47, (4.594 secs), cent 0.042505, acc 0.987700\n",
            "Epoch 47 start, learning rate 0.000100\n",
            "n_active: [365, 200, 155]\n",
            "\n",
            "Epoch 48 start, learning rate 0.000100\n",
            "train: epoch 48, (4.329 secs), cent 0.015219, acc 0.995333\n",
            "test: epoch 48, (4.568 secs), cent 0.042435, acc 0.987800\n",
            "Epoch 48 start, learning rate 0.000100\n",
            "n_active: [365, 200, 155]\n",
            "\n",
            "Epoch 49 start, learning rate 0.000100\n",
            "train: epoch 49, (4.401 secs), cent 0.014717, acc 0.995800\n",
            "test: epoch 49, (4.641 secs), cent 0.042467, acc 0.987800\n",
            "Epoch 49 start, learning rate 0.000100\n",
            "n_active: [365, 200, 155]\n",
            "\n",
            "Epoch 50 start, learning rate 0.000100\n",
            "train: epoch 50, (4.382 secs), cent 0.014711, acc 0.995767\n",
            "test: epoch 50, (4.620 secs), cent 0.042461, acc 0.988000\n",
            "Epoch 50 start, learning rate 0.000100\n",
            "n_active: [365, 200, 155]\n",
            "\n",
            "Epoch 51 start, learning rate 0.000100\n",
            "train: epoch 51, (4.307 secs), cent 0.015504, acc 0.995267\n",
            "test: epoch 51, (4.547 secs), cent 0.042198, acc 0.988100\n",
            "Epoch 51 start, learning rate 0.000100\n",
            "n_active: [365, 200, 154]\n",
            "\n",
            "Epoch 52 start, learning rate 0.000100\n",
            "train: epoch 52, (4.364 secs), cent 0.014605, acc 0.995967\n",
            "test: epoch 52, (4.610 secs), cent 0.042211, acc 0.988200\n",
            "Epoch 52 start, learning rate 0.000100\n",
            "n_active: [365, 200, 154]\n",
            "\n",
            "Epoch 53 start, learning rate 0.000100\n",
            "train: epoch 53, (4.394 secs), cent 0.014478, acc 0.996083\n",
            "test: epoch 53, (4.644 secs), cent 0.042279, acc 0.988300\n",
            "Epoch 53 start, learning rate 0.000100\n",
            "n_active: [365, 200, 153]\n",
            "\n",
            "Epoch 54 start, learning rate 0.000100\n",
            "train: epoch 54, (4.343 secs), cent 0.014823, acc 0.995867\n",
            "test: epoch 54, (4.583 secs), cent 0.042410, acc 0.988000\n",
            "Epoch 54 start, learning rate 0.000100\n",
            "n_active: [364, 200, 153]\n",
            "\n",
            "Epoch 55 start, learning rate 0.000100\n",
            "train: epoch 55, (4.331 secs), cent 0.014808, acc 0.995683\n",
            "test: epoch 55, (4.571 secs), cent 0.042439, acc 0.988100\n",
            "Epoch 55 start, learning rate 0.000100\n",
            "n_active: [363, 200, 153]\n",
            "\n",
            "Epoch 56 start, learning rate 0.000100\n",
            "train: epoch 56, (4.347 secs), cent 0.014492, acc 0.995733\n",
            "test: epoch 56, (4.587 secs), cent 0.042288, acc 0.988200\n",
            "Epoch 56 start, learning rate 0.000100\n",
            "n_active: [363, 200, 153]\n",
            "\n",
            "Epoch 57 start, learning rate 0.000100\n",
            "train: epoch 57, (4.375 secs), cent 0.014784, acc 0.995617\n",
            "test: epoch 57, (4.615 secs), cent 0.042267, acc 0.988000\n",
            "Epoch 57 start, learning rate 0.000100\n",
            "n_active: [363, 200, 153]\n",
            "\n",
            "Epoch 58 start, learning rate 0.000100\n",
            "train: epoch 58, (4.374 secs), cent 0.014066, acc 0.996200\n",
            "test: epoch 58, (4.603 secs), cent 0.042256, acc 0.988100\n",
            "Epoch 58 start, learning rate 0.000100\n",
            "n_active: [363, 200, 153]\n",
            "\n",
            "Epoch 59 start, learning rate 0.000100\n",
            "train: epoch 59, (4.304 secs), cent 0.014299, acc 0.996150\n",
            "test: epoch 59, (4.537 secs), cent 0.042477, acc 0.988100\n",
            "Epoch 59 start, learning rate 0.000100\n",
            "n_active: [363, 200, 153]\n",
            "\n",
            "Epoch 60 start, learning rate 0.000100\n",
            "train: epoch 60, (4.351 secs), cent 0.014506, acc 0.995667\n",
            "test: epoch 60, (4.583 secs), cent 0.042585, acc 0.988100\n",
            "Epoch 60 start, learning rate 0.000100\n",
            "n_active: [363, 200, 153]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O72sZ36dIosC",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYDT-eQ1kwQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "8237aecb-6ee3-4cba-8bbe-886e8f13ae3c"
      },
      "source": [
        "def test():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "    logger = Accumulator('cent', 'acc')\n",
        "    to_run = [tnet['cent'], tnet['acc']]\n",
        "    for j in range(n_test_batches):\n",
        "        bx, by = mnist.test.next_batch(batch_size)\n",
        "        logger.accum(sess.run(to_run, {x:bx, y:by}))\n",
        "    logger.print_(header='test')\n",
        "    \n",
        "    n_active = sess.run(tnet['n_active'])\n",
        "    print(\"The percentage of activated neurons per layer:\")\n",
        "    for na, nl in zip(n_active, [784, 500, 300]):\n",
        "      print('{}/{} = {:.2f}%'.format(na, nl, float(na)/nl * 100))\n",
        "    \n",
        "test()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test: cent 0.042585, acc 0.988100\n",
            "The percentage of activated neurons per layer:\n",
            "363/784 = 46.30%\n",
            "200/500 = 40.00%\n",
            "153/300 = 51.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8y4uwf-IqiR",
        "colab_type": "text"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0xBULDNoNG_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "76b9d717-5236-4fdb-8aed-1dbb14cba691"
      },
      "source": [
        "def visualize():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    n_drop = len(tnet['n_active'])\n",
        "    fig = figure('pi', figsize=(8,6))\n",
        "    axarr = fig.subplots(n_drop)\n",
        "    for i in range(n_drop):\n",
        "        np_pi = sess.run(tnet['pi'][i]).reshape((1,-1))\n",
        "        im = axarr[i].imshow(np_pi, cmap='Blues', aspect='auto')\n",
        "        axarr[i].yaxis.set_visible(False)\n",
        "        axarr[i].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        if i == n_drop-1:\n",
        "            axarr[i].set_xlabel('The Number of Neurons\\nLeNet [784, 500, 300]')\n",
        "        fig.colorbar(im, ax=axarr[i])\n",
        "    show()\n",
        "visualize()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGCCAYAAABaRzyuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x432 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOcZDWwRVyN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}